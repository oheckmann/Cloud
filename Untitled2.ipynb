{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1HK-o5GfwY36QnWIWBOjrOt2IG5fAhJht",
      "authorship_tag": "ABX9TyPNGxjpSgQ70ADLsrQLF42o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f686586819c44b09961a4d75b29fd515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2e0427e52cd841bc8bbee207ae9963c4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_73d9d9eeb5384c96bc002cf9583f9544",
              "IPY_MODEL_396018d11ac54bfe9bb85bcdeffc4a31",
              "IPY_MODEL_8581677fe4954a48ab3b16a3c3946790"
            ]
          }
        },
        "2e0427e52cd841bc8bbee207ae9963c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "73d9d9eeb5384c96bc002cf9583f9544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_69a80b8f4dcb4da4bdad2b84c5906e78",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_76fcfd6cff38461dac1961606e512fc5"
          }
        },
        "396018d11ac54bfe9bb85bcdeffc4a31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7f0c210f698d49129dd6b70ddc7e5878",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7b0812ff6407413b869a25ff9f0277d6"
          }
        },
        "8581677fe4954a48ab3b16a3c3946790": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_04c98f1bc1ff4a72b85a2b4a29e872c0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:02&lt;00:00, 58280733.68it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5cd938bb1d0e42e1a1816b33ca526d9d"
          }
        },
        "69a80b8f4dcb4da4bdad2b84c5906e78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "76fcfd6cff38461dac1961606e512fc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f0c210f698d49129dd6b70ddc7e5878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7b0812ff6407413b869a25ff9f0277d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "04c98f1bc1ff4a72b85a2b4a29e872c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5cd938bb1d0e42e1a1816b33ca526d9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oheckmann/Cloud/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGkUn6OYaZFd",
        "outputId": "ee332e87-16e7-4389-95f5-2dd0eb4b4322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[7.42e+03, 4.00e+00, 2.00e+00, 3.00e+00, 2.00e+00],\n",
              "       [8.96e+03, 4.00e+00, 4.00e+00, 4.00e+00, 3.00e+00],\n",
              "       [9.96e+03, 3.00e+00, 2.00e+00, 2.00e+00, 2.00e+00],\n",
              "       ...,\n",
              "       [3.62e+03, 2.00e+00, 1.00e+00, 1.00e+00, 0.00e+00],\n",
              "       [2.91e+03, 3.00e+00, 1.00e+00, 1.00e+00, 0.00e+00],\n",
              "       [3.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 0.00e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "house_path = \"drive/MyDrive/images/out.csv\"\n",
        "houseq_numpy2 = np.loadtxt(house_path, dtype=np.float, delimiter=\",\",\n",
        "skiprows=1, usecols=[1, 2,3,4,5])\n",
        "houseq_numpy2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "housing = pd.DataFrame(pd.read_csv(\"drive/MyDrive/images/Housing.csv\")) \n",
        "housing.head() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "oCEA0NrGgrZ5",
        "outputId": "d1349121-a0d9-462d-a9f9-30555195f190"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ab92cdf1-7e6c-40d6-a297-9d677bcc11c3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>area</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>stories</th>\n",
              "      <th>mainroad</th>\n",
              "      <th>guestroom</th>\n",
              "      <th>basement</th>\n",
              "      <th>hotwaterheating</th>\n",
              "      <th>airconditioning</th>\n",
              "      <th>parking</th>\n",
              "      <th>prefarea</th>\n",
              "      <th>furnishingstatus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>13300000</td>\n",
              "      <td>7420</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12250000</td>\n",
              "      <td>8960</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "      <td>no</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12250000</td>\n",
              "      <td>9960</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>semi-furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12215000</td>\n",
              "      <td>7500</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "      <td>yes</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11410000</td>\n",
              "      <td>7420</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "      <td>no</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab92cdf1-7e6c-40d6-a297-9d677bcc11c3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ab92cdf1-7e6c-40d6-a297-9d677bcc11c3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ab92cdf1-7e6c-40d6-a297-9d677bcc11c3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      price  area  bedrooms  ...  parking  prefarea furnishingstatus\n",
              "0  13300000  7420         4  ...        2       yes        furnished\n",
              "1  12250000  8960         4  ...        3        no        furnished\n",
              "2  12250000  9960         3  ...        2       yes   semi-furnished\n",
              "3  12215000  7500         4  ...        3       yes        furnished\n",
              "4  11410000  7420         4  ...        2        no        furnished\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking','price'] \n",
        "Newtrain = housing[num_vars] \n",
        "Newtrain.head() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "p_MelbhVgsWl",
        "outputId": "cab8f41e-8eca-47b3-9d6e-2a7fb390c336"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7a2de532-d244-4fc2-8f28-490d4b157c4f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>area</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>stories</th>\n",
              "      <th>parking</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7420</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>13300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8960</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>12250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9960</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>12250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7500</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>12215000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7420</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>11410000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a2de532-d244-4fc2-8f28-490d4b157c4f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7a2de532-d244-4fc2-8f28-490d4b157c4f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7a2de532-d244-4fc2-8f28-490d4b157c4f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   area  bedrooms  bathrooms  stories  parking     price\n",
              "0  7420         4          2        3        2  13300000\n",
              "1  8960         4          4        4        3  12250000\n",
              "2  9960         3          2        2        2  12250000\n",
              "3  7500         4          2        2        3  12215000\n",
              "4  7420         4          1        2        2  11410000"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "house_path = \"drive/MyDrive/images/out.csv\"\n",
        "houseq_numpy = np.loadtxt(house_path, dtype=np.float, delimiter=\",\",\n",
        "skiprows=1, usecols=[6])\n",
        "houseq_numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHVBHfX7wcTh",
        "outputId": "2b8f18d5-4372-412c-9272-89aa34fa2340"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([13300000., 12250000., 12250000., 12215000., 11410000., 10850000.,\n",
              "       10150000., 10150000.,  9870000.,  9800000.,  9800000.,  9681000.,\n",
              "        9310000.,  9240000.,  9240000.,  9100000.,  9100000.,  8960000.,\n",
              "        8890000.,  8855000.,  8750000.,  8680000.,  8645000.,  8645000.,\n",
              "        8575000.,  8540000.,  8463000.,  8400000.,  8400000.,  8400000.,\n",
              "        8400000.,  8400000.,  8295000.,  8190000.,  8120000.,  8080940.,\n",
              "        8043000.,  7980000.,  7962500.,  7910000.,  7875000.,  7840000.,\n",
              "        7700000.,  7700000.,  7560000.,  7560000.,  7525000.,  7490000.,\n",
              "        7455000.,  7420000.,  7420000.,  7420000.,  7350000.,  7350000.,\n",
              "        7350000.,  7350000.,  7343000.,  7245000.,  7210000.,  7210000.,\n",
              "        7140000.,  7070000.,  7070000.,  7035000.,  7000000.,  6930000.,\n",
              "        6930000.,  6895000.,  6860000.,  6790000.,  6790000.,  6755000.,\n",
              "        6720000.,  6685000.,  6650000.,  6650000.,  6650000.,  6650000.,\n",
              "        6650000.,  6650000.,  6629000.,  6615000.,  6615000.,  6580000.,\n",
              "        6510000.,  6510000.,  6510000.,  6475000.,  6475000.,  6440000.,\n",
              "        6440000.,  6419000.,  6405000.,  6300000.,  6300000.,  6300000.,\n",
              "        6300000.,  6300000.,  6293000.,  6265000.,  6230000.,  6230000.,\n",
              "        6195000.,  6195000.,  6195000.,  6160000.,  6160000.,  6125000.,\n",
              "        6107500.,  6090000.,  6090000.,  6090000.,  6083000.,  6083000.,\n",
              "        6020000.,  6020000.,  6020000.,  5950000.,  5950000.,  5950000.,\n",
              "        5950000.,  5950000.,  5950000.,  5950000.,  5950000.,  5943000.,\n",
              "        5880000.,  5880000.,  5873000.,  5873000.,  5866000.,  5810000.,\n",
              "        5810000.,  5810000.,  5803000.,  5775000.,  5740000.,  5740000.,\n",
              "        5740000.,  5740000.,  5740000.,  5652500.,  5600000.,  5600000.,\n",
              "        5600000.,  5600000.,  5600000.,  5600000.,  5600000.,  5600000.,\n",
              "        5600000.,  5565000.,  5565000.,  5530000.,  5530000.,  5530000.,\n",
              "        5523000.,  5495000.,  5495000.,  5460000.,  5460000.,  5460000.,\n",
              "        5460000.,  5425000.,  5390000.,  5383000.,  5320000.,  5285000.,\n",
              "        5250000.,  5250000.,  5250000.,  5250000.,  5250000.,  5250000.,\n",
              "        5250000.,  5250000.,  5250000.,  5243000.,  5229000.,  5215000.,\n",
              "        5215000.,  5215000.,  5145000.,  5145000.,  5110000.,  5110000.,\n",
              "        5110000.,  5110000.,  5075000.,  5040000.,  5040000.,  5040000.,\n",
              "        5040000.,  5033000.,  5005000.,  4970000.,  4970000.,  4956000.,\n",
              "        4935000.,  4907000.,  4900000.,  4900000.,  4900000.,  4900000.,\n",
              "        4900000.,  4900000.,  4900000.,  4900000.,  4900000.,  4900000.,\n",
              "        4900000.,  4900000.,  4893000.,  4893000.,  4865000.,  4830000.,\n",
              "        4830000.,  4830000.,  4830000.,  4795000.,  4795000.,  4767000.,\n",
              "        4760000.,  4760000.,  4760000.,  4753000.,  4690000.,  4690000.,\n",
              "        4690000.,  4690000.,  4690000.,  4690000.,  4655000.,  4620000.,\n",
              "        4620000.,  4620000.,  4620000.,  4620000.,  4613000.,  4585000.,\n",
              "        4585000.,  4550000.,  4550000.,  4550000.,  4550000.,  4550000.,\n",
              "        4550000.,  4550000.,  4543000.,  4543000.,  4515000.,  4515000.,\n",
              "        4515000.,  4515000.,  4480000.,  4480000.,  4480000.,  4480000.,\n",
              "        4480000.,  4473000.,  4473000.,  4473000.,  4445000.,  4410000.,\n",
              "        4410000.,  4403000.,  4403000.,  4403000.,  4382000.,  4375000.,\n",
              "        4340000.,  4340000.,  4340000.,  4340000.,  4340000.,  4319000.,\n",
              "        4305000.,  4305000.,  4277000.,  4270000.,  4270000.,  4270000.,\n",
              "        4270000.,  4270000.,  4270000.,  4235000.,  4235000.,  4200000.,\n",
              "        4200000.,  4200000.,  4200000.,  4200000.,  4200000.,  4200000.,\n",
              "        4200000.,  4200000.,  4200000.,  4200000.,  4200000.,  4200000.,\n",
              "        4200000.,  4200000.,  4200000.,  4200000.,  4193000.,  4193000.,\n",
              "        4165000.,  4165000.,  4165000.,  4130000.,  4130000.,  4123000.,\n",
              "        4098500.,  4095000.,  4095000.,  4095000.,  4060000.,  4060000.,\n",
              "        4060000.,  4060000.,  4060000.,  4025000.,  4025000.,  4025000.,\n",
              "        4007500.,  4007500.,  3990000.,  3990000.,  3990000.,  3990000.,\n",
              "        3990000.,  3920000.,  3920000.,  3920000.,  3920000.,  3920000.,\n",
              "        3920000.,  3920000.,  3885000.,  3885000.,  3850000.,  3850000.,\n",
              "        3850000.,  3850000.,  3850000.,  3850000.,  3850000.,  3836000.,\n",
              "        3815000.,  3780000.,  3780000.,  3780000.,  3780000.,  3780000.,\n",
              "        3780000.,  3773000.,  3773000.,  3773000.,  3745000.,  3710000.,\n",
              "        3710000.,  3710000.,  3710000.,  3710000.,  3703000.,  3703000.,\n",
              "        3675000.,  3675000.,  3675000.,  3675000.,  3640000.,  3640000.,\n",
              "        3640000.,  3640000.,  3640000.,  3640000.,  3640000.,  3640000.,\n",
              "        3640000.,  3633000.,  3605000.,  3605000.,  3570000.,  3570000.,\n",
              "        3570000.,  3570000.,  3535000.,  3500000.,  3500000.,  3500000.,\n",
              "        3500000.,  3500000.,  3500000.,  3500000.,  3500000.,  3500000.,\n",
              "        3500000.,  3500000.,  3500000.,  3500000.,  3500000.,  3500000.,\n",
              "        3500000.,  3500000.,  3493000.,  3465000.,  3465000.,  3465000.,\n",
              "        3430000.,  3430000.,  3430000.,  3430000.,  3430000.,  3430000.,\n",
              "        3423000.,  3395000.,  3395000.,  3395000.,  3360000.,  3360000.,\n",
              "        3360000.,  3360000.,  3360000.,  3360000.,  3360000.,  3360000.,\n",
              "        3353000.,  3332000.,  3325000.,  3325000.,  3290000.,  3290000.,\n",
              "        3290000.,  3290000.,  3290000.,  3290000.,  3290000.,  3290000.,\n",
              "        3255000.,  3255000.,  3234000.,  3220000.,  3220000.,  3220000.,\n",
              "        3220000.,  3150000.,  3150000.,  3150000.,  3150000.,  3150000.,\n",
              "        3150000.,  3150000.,  3150000.,  3150000.,  3143000.,  3129000.,\n",
              "        3118850.,  3115000.,  3115000.,  3115000.,  3087000.,  3080000.,\n",
              "        3080000.,  3080000.,  3080000.,  3045000.,  3010000.,  3010000.,\n",
              "        3010000.,  3010000.,  3010000.,  3010000.,  3010000.,  3003000.,\n",
              "        2975000.,  2961000.,  2940000.,  2940000.,  2940000.,  2940000.,\n",
              "        2940000.,  2940000.,  2940000.,  2940000.,  2870000.,  2870000.,\n",
              "        2870000.,  2870000.,  2852500.,  2835000.,  2835000.,  2835000.,\n",
              "        2800000.,  2800000.,  2730000.,  2730000.,  2695000.,  2660000.,\n",
              "        2660000.,  2660000.,  2660000.,  2660000.,  2660000.,  2660000.,\n",
              "        2653000.,  2653000.,  2604000.,  2590000.,  2590000.,  2590000.,\n",
              "        2520000.,  2520000.,  2520000.,  2485000.,  2485000.,  2450000.,\n",
              "        2450000.,  2450000.,  2450000.,  2450000.,  2450000.,  2408000.,\n",
              "        2380000.,  2380000.,  2380000.,  2345000.,  2310000.,  2275000.,\n",
              "        2275000.,  2275000.,  2240000.,  2233000.,  2135000.,  2100000.,\n",
              "        2100000.,  2100000.,  1960000.,  1890000.,  1890000.,  1855000.,\n",
              "        1820000.,  1767150.,  1750000.,  1750000.,  1750000.])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tc = torch.from_numpy(houseq_numpy)\n",
        "tc = torch.tensor(tc.float()).unsqueeze(1)\n",
        "tc.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5Pmn9I92I0i",
        "outputId": "cbebbff9-9596-4042-93a0-29c5d18979c4"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([545, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tu = torch.from_numpy(houseq_numpy2)\n",
        "tu = torch.tensor(tu.float())\n",
        "tu.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gn14U4Z75SgL",
        "outputId": "2efe98b5-83bc-4dc0-c90b-de172a72aaf1"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([545, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
        "t_price = tc\n",
        "t_data = tu\n",
        "\n",
        "\n",
        "t_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jOIwVjFwg-6",
        "outputId": "b3af2031-8e19-480d-aa2a-95bad819e2b1"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([545, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(x):\n",
        "    x_normed = x / x.max(0, keepdim=True)[0]\n",
        "    return x_normed"
      ],
      "metadata": {
        "id": "-cYLgZyMWKYn"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_norm = normalize(t_data)\n",
        "tc = normalize(tc)"
      ],
      "metadata": {
        "id": "I234lJphWMlO"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = t_norm.shape[0]\n",
        "n_val = int(0.2 * n_samples)\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "train_indices = shuffled_indices[n_val:]\n",
        "val_indices = shuffled_indices[:n_val]\n",
        "train_indices, val_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYcohGhukcvH",
        "outputId": "72d93df5-4127-4beb-c432-8669977c281c"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 42, 486, 259, 375, 528, 490, 246, 117, 392, 466, 133, 349, 301,\n",
              "         152, 257,  99,  89, 146, 363, 213, 289, 127, 416, 185, 533, 120,\n",
              "         453,  23, 529, 422, 334, 116, 189, 467, 115, 112, 104, 457, 308,\n",
              "         148, 304,  19, 439, 474, 188, 193, 200, 153, 365, 295, 430, 159,\n",
              "         181,  74, 391, 150, 136, 288, 366,  59, 225, 297, 388, 138, 464,\n",
              "         421, 352, 239, 484,  63, 178, 218,  10, 539,   2,  82, 468,  33,\n",
              "         192, 161, 207, 433, 183, 508,  45, 252,  61, 426,  43,  86,  83,\n",
              "          91, 186, 300,  58,  94,  49, 217, 342, 241,  98, 488, 103, 437,\n",
              "         258, 458, 456, 465, 312, 541, 310, 478, 509, 335,  55, 267, 264,\n",
              "         273, 459, 503, 515, 532, 425, 125, 368, 173,  13,  20, 357, 121,\n",
              "         339, 197, 412,  80, 510, 374, 444, 482, 491, 455, 102, 299, 463,\n",
              "         543, 124, 327, 226, 526, 340, 254, 338, 477, 500, 397, 445, 134,\n",
              "         294, 511, 504,  81, 129, 160, 405,  17, 364, 540, 410, 326, 204,\n",
              "         245,  24, 263, 256, 460, 137, 244, 171, 473, 234, 235, 471, 272,\n",
              "         420, 336, 379, 278,  38, 266, 483, 195, 139, 494, 287,  32, 100,\n",
              "         158, 446, 123, 535, 525, 214, 281, 316,   0, 347, 516, 231,  66,\n",
              "          36, 461,  75, 454, 269, 107,  34, 434, 361,  26, 389,  54, 108,\n",
              "         242, 130, 230,  27, 330,   9, 527, 367,  46, 221, 250, 206, 305,\n",
              "         462,  29, 190,  69, 479, 408, 429, 210,  85, 536, 383,  25, 177,\n",
              "         377, 448, 387,  39, 382,  22, 302,  78,  18, 418, 147, 436, 518,\n",
              "         325, 332, 313, 400,  12, 154, 447, 255, 432, 247, 182, 328, 538,\n",
              "         440, 395, 222, 307, 292, 198, 311, 354, 318,  51, 450, 492,   4,\n",
              "         497, 348, 212, 296,  65, 343, 393, 275, 522, 110, 381,  92, 220,\n",
              "          35,  30, 279, 194, 227, 443, 469, 143, 211,  90, 401,  48, 163,\n",
              "         164, 441, 144, 407, 151,  84, 534, 411, 376, 498, 428, 499, 118,\n",
              "         142, 270, 215, 128, 271,  11,  44, 378, 224, 291, 345, 531,  93,\n",
              "         162, 414, 372,  37, 260, 362, 179, 228, 216, 285, 209, 240, 481,\n",
              "         358,  62,  60, 170, 219, 175, 521,  50, 157, 238,  14,  70,  21,\n",
              "          79,   1,  73, 180,  52, 293, 423,  71, 111, 315,  64, 236, 167,\n",
              "         280, 223, 394, 470, 141,  68,  67, 523, 199, 261, 544,  16, 517,\n",
              "         251, 514,  96, 323,  72, 344,   5, 530, 520, 419, 524, 166, 262,\n",
              "          87, 135, 169,  15, 208, 356, 321,   8, 413, 187, 404, 519,   3,\n",
              "         480, 317, 353,  76, 373, 174, 386, 268,  53, 201, 284, 290, 229,\n",
              "         114, 253, 350, 370,  97, 274, 105]),\n",
              " tensor([232, 176, 106, 322, 512, 351,  77, 276, 489, 485,  41, 346, 155,\n",
              "         402, 298, 355,  40, 309, 371, 390, 406, 320, 249, 282, 513, 132,\n",
              "           6, 501, 303, 384, 359,  47, 283, 487, 435, 324, 360, 427, 237,\n",
              "         233,  56, 314, 168, 380,  31, 149,  88, 329, 131, 286, 140, 341,\n",
              "         451, 184, 337, 205, 438, 502, 399, 145, 398, 202, 203, 507, 319,\n",
              "         396,  28, 449, 113, 505, 172, 119, 475, 472, 415, 493, 409, 196,\n",
              "         122, 403, 191, 265, 442,  95, 126, 243, 165, 424, 333,  57, 306,\n",
              "         109,   7, 385, 537, 369, 431, 452, 101, 506, 542, 476, 156, 248,\n",
              "         331, 495, 496, 277, 417]))"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_t_u = t_norm[train_indices]\n",
        "train_t_c = tc[train_indices]\n",
        "val_t_u = t_norm[val_indices]\n",
        "val_t_c = tc[val_indices]\n",
        "train_t_un = 0.1 * train_t_u\n",
        "val_t_un = 0.1 * val_t_u"
      ],
      "metadata": {
        "id": "Hoy3QalZo5su"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "from collections import OrderedDict\n",
        "seq_model = nn.Sequential(OrderedDict([\n",
        "('hidden_linear', nn.Linear(5, 8)),\n",
        "('hidden_activation', nn.Tanh()),\n",
        "('output_linear', nn.Linear(8, 1))\n",
        "]))\n",
        "seq_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TakeMVW28nJz",
        "outputId": "7ae7dcc6-238b-43ed-a70b-708be7c0a505"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (hidden_linear): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (hidden_activation): Tanh()\n",
              "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = seq_model(train_t_un)\n",
        "loss = nn.MSELoss()\n",
        "loss(d, train_t_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoZxSltKDISE",
        "outputId": "16afac5f-907b-4b2c-9df9-50bca540ffdf"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1200, grad_fn=<MseLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in seq_model.named_parameters():\n",
        "  print(name, param.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG4QEY_28wTX",
        "outputId": "1383c67e-e0b5-4327-f478-d08b67376e63"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden_linear.weight torch.Size([8, 5])\n",
            "hidden_linear.bias torch.Size([8])\n",
            "output_linear.weight torch.Size([1, 8])\n",
            "output_linear.bias torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[param.shape for param in seq_model.parameters()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHb7jUFu802g",
        "outputId": "cb2defc3-ed2a-4654-e7dd-22c0d3a30d2c"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([8, 5]), torch.Size([8]), torch.Size([1, 8]), torch.Size([1])]"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train, t_c_val):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    t_p_train = model(t_u_train)\n",
        "    loss_train = loss_fn(t_p_train, t_c_train)\n",
        "    t_p_val = model(t_u_val)\n",
        "    loss_val = loss_fn(t_p_val, t_c_val)\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    if epoch == 1 or epoch % 10 == 0:\n",
        "      print(f\"Epoch {epoch}, Training loss {loss_train.item():.4f},\" f\" Validation loss {loss_val.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "H7EheJDBi_2f"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(seq_model.parameters(), lr=1e-5)\n",
        "\n",
        "training_loop(\n",
        "n_epochs = 200,\n",
        "optimizer = optimizer,\n",
        "model = seq_model,\n",
        "loss_fn = nn.MSELoss(),\n",
        "t_u_train = train_t_un,\n",
        "t_u_val = val_t_un,\n",
        "t_c_train = train_t_c,\n",
        "t_c_val = val_t_c)\n",
        "seq_model.parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0a_DTBTjs00",
        "outputId": "d0e71b75-6d17-432e-e5f4-e6f1f625afd2"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 0.1200, Validation loss 0.0953\n",
            "Epoch 10, Training loss 0.1197, Validation loss 0.0951\n",
            "Epoch 20, Training loss 0.1188, Validation loss 0.0942\n",
            "Epoch 30, Training loss 0.1173, Validation loss 0.0929\n",
            "Epoch 40, Training loss 0.1152, Validation loss 0.0910\n",
            "Epoch 50, Training loss 0.1125, Validation loss 0.0886\n",
            "Epoch 60, Training loss 0.1093, Validation loss 0.0857\n",
            "Epoch 70, Training loss 0.1056, Validation loss 0.0823\n",
            "Epoch 80, Training loss 0.1014, Validation loss 0.0786\n",
            "Epoch 90, Training loss 0.0969, Validation loss 0.0746\n",
            "Epoch 100, Training loss 0.0920, Validation loss 0.0703\n",
            "Epoch 110, Training loss 0.0869, Validation loss 0.0657\n",
            "Epoch 120, Training loss 0.0815, Validation loss 0.0610\n",
            "Epoch 130, Training loss 0.0760, Validation loss 0.0562\n",
            "Epoch 140, Training loss 0.0704, Validation loss 0.0514\n",
            "Epoch 150, Training loss 0.0649, Validation loss 0.0466\n",
            "Epoch 160, Training loss 0.0594, Validation loss 0.0419\n",
            "Epoch 170, Training loss 0.0540, Validation loss 0.0375\n",
            "Epoch 180, Training loss 0.0489, Validation loss 0.0332\n",
            "Epoch 190, Training loss 0.0440, Validation loss 0.0293\n",
            "Epoch 200, Training loss 0.0395, Validation loss 0.0258\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of Sequential(\n",
              "  (hidden_linear): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (hidden_activation): Tanh()\n",
              "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_model(train_t_un)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3EylhwZ6TGM",
        "outputId": "0758fcff-cd3c-4c52-89ec-da77ecad5650"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2096],\n",
              "        [0.2373],\n",
              "        [0.2355],\n",
              "        [0.2334],\n",
              "        [0.2299],\n",
              "        [0.2291],\n",
              "        [0.2391],\n",
              "        [0.2354],\n",
              "        [0.2309],\n",
              "        [0.2298],\n",
              "        [0.2231],\n",
              "        [0.2319],\n",
              "        [0.2303],\n",
              "        [0.2424],\n",
              "        [0.2450],\n",
              "        [0.2295],\n",
              "        [0.2373],\n",
              "        [0.2407],\n",
              "        [0.2343],\n",
              "        [0.2435],\n",
              "        [0.2326],\n",
              "        [0.2229],\n",
              "        [0.2344],\n",
              "        [0.2297],\n",
              "        [0.2289],\n",
              "        [0.2383],\n",
              "        [0.2314],\n",
              "        [0.2270],\n",
              "        [0.2309],\n",
              "        [0.2345],\n",
              "        [0.2317],\n",
              "        [0.2410],\n",
              "        [0.2342],\n",
              "        [0.2254],\n",
              "        [0.2401],\n",
              "        [0.2436],\n",
              "        [0.2347],\n",
              "        [0.2384],\n",
              "        [0.2287],\n",
              "        [0.2251],\n",
              "        [0.2381],\n",
              "        [0.2294],\n",
              "        [0.2347],\n",
              "        [0.2339],\n",
              "        [0.2282],\n",
              "        [0.2358],\n",
              "        [0.2316],\n",
              "        [0.2254],\n",
              "        [0.2366],\n",
              "        [0.2288],\n",
              "        [0.2329],\n",
              "        [0.2363],\n",
              "        [0.2327],\n",
              "        [0.2287],\n",
              "        [0.2297],\n",
              "        [0.2323],\n",
              "        [0.2307],\n",
              "        [0.2322],\n",
              "        [0.2344],\n",
              "        [0.2112],\n",
              "        [0.2310],\n",
              "        [0.2281],\n",
              "        [0.2305],\n",
              "        [0.2234],\n",
              "        [0.2244],\n",
              "        [0.2358],\n",
              "        [0.2326],\n",
              "        [0.2286],\n",
              "        [0.2336],\n",
              "        [0.2231],\n",
              "        [0.2390],\n",
              "        [0.2358],\n",
              "        [0.2379],\n",
              "        [0.2313],\n",
              "        [0.2316],\n",
              "        [0.2433],\n",
              "        [0.2334],\n",
              "        [0.2265],\n",
              "        [0.2430],\n",
              "        [0.2247],\n",
              "        [0.2297],\n",
              "        [0.2329],\n",
              "        [0.2424],\n",
              "        [0.2353],\n",
              "        [0.2223],\n",
              "        [0.2471],\n",
              "        [0.2387],\n",
              "        [0.2381],\n",
              "        [0.2139],\n",
              "        [0.2255],\n",
              "        [0.2135],\n",
              "        [0.2337],\n",
              "        [0.2353],\n",
              "        [0.2222],\n",
              "        [0.2182],\n",
              "        [0.2161],\n",
              "        [0.2417],\n",
              "        [0.2300],\n",
              "        [0.2349],\n",
              "        [0.2348],\n",
              "        [0.2230],\n",
              "        [0.2285],\n",
              "        [0.2227],\n",
              "        [0.2398],\n",
              "        [0.2287],\n",
              "        [0.2307],\n",
              "        [0.2377],\n",
              "        [0.2346],\n",
              "        [0.2392],\n",
              "        [0.2377],\n",
              "        [0.2422],\n",
              "        [0.2281],\n",
              "        [0.2210],\n",
              "        [0.2300],\n",
              "        [0.2312],\n",
              "        [0.2298],\n",
              "        [0.2272],\n",
              "        [0.2306],\n",
              "        [0.2342],\n",
              "        [0.2397],\n",
              "        [0.2299],\n",
              "        [0.2336],\n",
              "        [0.2292],\n",
              "        [0.2497],\n",
              "        [0.2369],\n",
              "        [0.2439],\n",
              "        [0.2283],\n",
              "        [0.2268],\n",
              "        [0.2372],\n",
              "        [0.2350],\n",
              "        [0.2325],\n",
              "        [0.2248],\n",
              "        [0.2292],\n",
              "        [0.2312],\n",
              "        [0.2383],\n",
              "        [0.2319],\n",
              "        [0.2298],\n",
              "        [0.2298],\n",
              "        [0.2308],\n",
              "        [0.2416],\n",
              "        [0.2106],\n",
              "        [0.2366],\n",
              "        [0.2337],\n",
              "        [0.2383],\n",
              "        [0.2119],\n",
              "        [0.2318],\n",
              "        [0.2148],\n",
              "        [0.2338],\n",
              "        [0.2400],\n",
              "        [0.2319],\n",
              "        [0.2257],\n",
              "        [0.2360],\n",
              "        [0.2382],\n",
              "        [0.2349],\n",
              "        [0.2292],\n",
              "        [0.2389],\n",
              "        [0.2335],\n",
              "        [0.2387],\n",
              "        [0.2338],\n",
              "        [0.2263],\n",
              "        [0.2270],\n",
              "        [0.2210],\n",
              "        [0.2385],\n",
              "        [0.2122],\n",
              "        [0.2298],\n",
              "        [0.2290],\n",
              "        [0.2395],\n",
              "        [0.2447],\n",
              "        [0.2407],\n",
              "        [0.2281],\n",
              "        [0.2301],\n",
              "        [0.2309],\n",
              "        [0.2264],\n",
              "        [0.2377],\n",
              "        [0.2343],\n",
              "        [0.2326],\n",
              "        [0.2453],\n",
              "        [0.2399],\n",
              "        [0.2239],\n",
              "        [0.2308],\n",
              "        [0.2306],\n",
              "        [0.2352],\n",
              "        [0.2262],\n",
              "        [0.2402],\n",
              "        [0.2345],\n",
              "        [0.2256],\n",
              "        [0.2113],\n",
              "        [0.2362],\n",
              "        [0.2342],\n",
              "        [0.2272],\n",
              "        [0.2381],\n",
              "        [0.2383],\n",
              "        [0.2328],\n",
              "        [0.2323],\n",
              "        [0.2407],\n",
              "        [0.2401],\n",
              "        [0.2302],\n",
              "        [0.2377],\n",
              "        [0.2317],\n",
              "        [0.2344],\n",
              "        [0.2353],\n",
              "        [0.2309],\n",
              "        [0.2336],\n",
              "        [0.2245],\n",
              "        [0.2301],\n",
              "        [0.2316],\n",
              "        [0.2401],\n",
              "        [0.2441],\n",
              "        [0.2219],\n",
              "        [0.2360],\n",
              "        [0.2338],\n",
              "        [0.2315],\n",
              "        [0.2308],\n",
              "        [0.2251],\n",
              "        [0.2419],\n",
              "        [0.2355],\n",
              "        [0.2422],\n",
              "        [0.2135],\n",
              "        [0.2365],\n",
              "        [0.2289],\n",
              "        [0.2238],\n",
              "        [0.2305],\n",
              "        [0.2407],\n",
              "        [0.2278],\n",
              "        [0.2435],\n",
              "        [0.2261],\n",
              "        [0.2109],\n",
              "        [0.2321],\n",
              "        [0.2344],\n",
              "        [0.2112],\n",
              "        [0.2328],\n",
              "        [0.2215],\n",
              "        [0.2371],\n",
              "        [0.2280],\n",
              "        [0.2286],\n",
              "        [0.2331],\n",
              "        [0.2357],\n",
              "        [0.2391],\n",
              "        [0.2354],\n",
              "        [0.2348],\n",
              "        [0.2368],\n",
              "        [0.2272],\n",
              "        [0.2251],\n",
              "        [0.2399],\n",
              "        [0.2296],\n",
              "        [0.2321],\n",
              "        [0.2423],\n",
              "        [0.2272],\n",
              "        [0.2304],\n",
              "        [0.2312],\n",
              "        [0.2161],\n",
              "        [0.2298],\n",
              "        [0.2425],\n",
              "        [0.2175],\n",
              "        [0.2372],\n",
              "        [0.2248],\n",
              "        [0.2306],\n",
              "        [0.2282],\n",
              "        [0.2286],\n",
              "        [0.2342],\n",
              "        [0.2351],\n",
              "        [0.2377],\n",
              "        [0.2282],\n",
              "        [0.2319],\n",
              "        [0.2344],\n",
              "        [0.2236],\n",
              "        [0.2342],\n",
              "        [0.2349],\n",
              "        [0.2423],\n",
              "        [0.2170],\n",
              "        [0.2302],\n",
              "        [0.2292],\n",
              "        [0.2344],\n",
              "        [0.2353],\n",
              "        [0.2427],\n",
              "        [0.2367],\n",
              "        [0.2265],\n",
              "        [0.2319],\n",
              "        [0.2334],\n",
              "        [0.2351],\n",
              "        [0.2381],\n",
              "        [0.2401],\n",
              "        [0.2140],\n",
              "        [0.2302],\n",
              "        [0.2269],\n",
              "        [0.2355],\n",
              "        [0.2347],\n",
              "        [0.2314],\n",
              "        [0.2282],\n",
              "        [0.2271],\n",
              "        [0.2325],\n",
              "        [0.2349],\n",
              "        [0.2440],\n",
              "        [0.2349],\n",
              "        [0.2290],\n",
              "        [0.2384],\n",
              "        [0.2348],\n",
              "        [0.2120],\n",
              "        [0.2188],\n",
              "        [0.2103],\n",
              "        [0.2109],\n",
              "        [0.2378],\n",
              "        [0.2426],\n",
              "        [0.2351],\n",
              "        [0.2378],\n",
              "        [0.2356],\n",
              "        [0.2306],\n",
              "        [0.2463],\n",
              "        [0.2322],\n",
              "        [0.2310],\n",
              "        [0.2267],\n",
              "        [0.2432],\n",
              "        [0.2297],\n",
              "        [0.2314],\n",
              "        [0.2344],\n",
              "        [0.2198],\n",
              "        [0.2317],\n",
              "        [0.2261],\n",
              "        [0.2345],\n",
              "        [0.2198],\n",
              "        [0.2242],\n",
              "        [0.2235],\n",
              "        [0.2326],\n",
              "        [0.2258],\n",
              "        [0.2427],\n",
              "        [0.2394],\n",
              "        [0.2182],\n",
              "        [0.2223],\n",
              "        [0.2217],\n",
              "        [0.2380],\n",
              "        [0.2292],\n",
              "        [0.2161],\n",
              "        [0.2199],\n",
              "        [0.2381],\n",
              "        [0.2296],\n",
              "        [0.2284],\n",
              "        [0.2413],\n",
              "        [0.2346],\n",
              "        [0.2368],\n",
              "        [0.2349],\n",
              "        [0.2299],\n",
              "        [0.2177],\n",
              "        [0.2276],\n",
              "        [0.2349],\n",
              "        [0.2231],\n",
              "        [0.2259],\n",
              "        [0.2377],\n",
              "        [0.2342],\n",
              "        [0.2431],\n",
              "        [0.2284],\n",
              "        [0.2332],\n",
              "        [0.2342],\n",
              "        [0.2340],\n",
              "        [0.2289],\n",
              "        [0.2393],\n",
              "        [0.2347],\n",
              "        [0.2362],\n",
              "        [0.2344],\n",
              "        [0.2130],\n",
              "        [0.2297],\n",
              "        [0.2341],\n",
              "        [0.2334],\n",
              "        [0.2286],\n",
              "        [0.2368],\n",
              "        [0.2223],\n",
              "        [0.2107],\n",
              "        [0.2093],\n",
              "        [0.2383],\n",
              "        [0.2161],\n",
              "        [0.2331],\n",
              "        [0.2394],\n",
              "        [0.2184],\n",
              "        [0.2231],\n",
              "        [0.2368],\n",
              "        [0.2464],\n",
              "        [0.2246],\n",
              "        [0.2287],\n",
              "        [0.2290],\n",
              "        [0.2316],\n",
              "        [0.2390],\n",
              "        [0.2410],\n",
              "        [0.2346],\n",
              "        [0.2399],\n",
              "        [0.2375],\n",
              "        [0.2320],\n",
              "        [0.2289],\n",
              "        [0.2303],\n",
              "        [0.2307],\n",
              "        [0.2345],\n",
              "        [0.2313],\n",
              "        [0.2280],\n",
              "        [0.2297],\n",
              "        [0.2437],\n",
              "        [0.2346],\n",
              "        [0.2146],\n",
              "        [0.2346],\n",
              "        [0.2350],\n",
              "        [0.2371],\n",
              "        [0.2395],\n",
              "        [0.2282],\n",
              "        [0.2339],\n",
              "        [0.2399],\n",
              "        [0.2345],\n",
              "        [0.2351],\n",
              "        [0.2135],\n",
              "        [0.2367],\n",
              "        [0.2337],\n",
              "        [0.2208],\n",
              "        [0.2430],\n",
              "        [0.2236],\n",
              "        [0.2364],\n",
              "        [0.2260],\n",
              "        [0.2423],\n",
              "        [0.2320],\n",
              "        [0.2271],\n",
              "        [0.2311],\n",
              "        [0.2280],\n",
              "        [0.2253],\n",
              "        [0.2207],\n",
              "        [0.2228],\n",
              "        [0.2248],\n",
              "        [0.2284],\n",
              "        [0.2349],\n",
              "        [0.2370],\n",
              "        [0.2080],\n",
              "        [0.2310],\n",
              "        [0.2373],\n",
              "        [0.2294],\n",
              "        [0.2383],\n",
              "        [0.2338],\n",
              "        [0.2254],\n",
              "        [0.2230],\n",
              "        [0.2306],\n",
              "        [0.2404],\n",
              "        [0.2389],\n",
              "        [0.2139]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "seq_model2 = nn.Sequential(OrderedDict([\n",
        "('hidden_linear', nn.Linear(5, 8)),\n",
        "('hidden_activation', nn.Tanh()),\n",
        "('hidden_linear2', nn.Linear(8, 8)),\n",
        "('hidden_activation2', nn.Tanh()),\n",
        "('output_linear', nn.Linear(8, 1))\n",
        "]))\n",
        "seq_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMxB-752qQBO",
        "outputId": "8381208c-2de2-462a-f763-1b3dec16095a"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (hidden_linear): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (hidden_activation): Tanh()\n",
              "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(seq_model2.parameters(), lr=1e-5)\n",
        "\n",
        "training_loop(\n",
        "n_epochs = 200,\n",
        "optimizer = optimizer,\n",
        "model = seq_model2,\n",
        "loss_fn = nn.MSELoss(),\n",
        "t_u_train = train_t_un,\n",
        "t_u_val = val_t_un,\n",
        "t_c_train = train_t_c,\n",
        "t_c_val = val_t_c)\n",
        "seq_model.parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK5zxdum8buj",
        "outputId": "269e2864-98a8-43f9-d7ac-039209920f3f"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 0.0387, Validation loss 0.0254\n",
            "Epoch 10, Training loss 0.0386, Validation loss 0.0254\n",
            "Epoch 20, Training loss 0.0384, Validation loss 0.0252\n",
            "Epoch 30, Training loss 0.0380, Validation loss 0.0249\n",
            "Epoch 40, Training loss 0.0375, Validation loss 0.0245\n",
            "Epoch 50, Training loss 0.0369, Validation loss 0.0240\n",
            "Epoch 60, Training loss 0.0361, Validation loss 0.0235\n",
            "Epoch 70, Training loss 0.0352, Validation loss 0.0228\n",
            "Epoch 80, Training loss 0.0343, Validation loss 0.0221\n",
            "Epoch 90, Training loss 0.0332, Validation loss 0.0213\n",
            "Epoch 100, Training loss 0.0321, Validation loss 0.0205\n",
            "Epoch 110, Training loss 0.0310, Validation loss 0.0196\n",
            "Epoch 120, Training loss 0.0298, Validation loss 0.0188\n",
            "Epoch 130, Training loss 0.0286, Validation loss 0.0180\n",
            "Epoch 140, Training loss 0.0275, Validation loss 0.0173\n",
            "Epoch 150, Training loss 0.0263, Validation loss 0.0166\n",
            "Epoch 160, Training loss 0.0253, Validation loss 0.0159\n",
            "Epoch 170, Training loss 0.0243, Validation loss 0.0154\n",
            "Epoch 180, Training loss 0.0234, Validation loss 0.0150\n",
            "Epoch 190, Training loss 0.0226, Validation loss 0.0147\n",
            "Epoch 200, Training loss 0.0219, Validation loss 0.0145\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of Sequential(\n",
              "  (hidden_linear): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (hidden_activation): Tanh()\n",
              "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "seq_model3 = nn.Sequential(OrderedDict([\n",
        "('hidden_linear', nn.Linear(5, 8)),\n",
        "('hidden_activation', nn.Tanh()),\n",
        "('hidden_linear2', nn.Linear(8, 16)),\n",
        "('hidden_activation2', nn.Tanh()),\n",
        "('hidden_linear3', nn.Linear(16, 8)),\n",
        "('hidden_activation3', nn.Tanh()),\n",
        "('output_linear', nn.Linear(8, 1))\n",
        "]))\n",
        "seq_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mxJ-iHj82cf",
        "outputId": "4c62c74d-3191-427a-dfe7-7e7cefe2c0e6"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (hidden_linear): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (hidden_activation): Tanh()\n",
              "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(seq_model3.parameters(), lr=1e-5)\n",
        "\n",
        "training_loop(\n",
        "n_epochs = 200,\n",
        "optimizer = optimizer,\n",
        "model = seq_model3,\n",
        "loss_fn = nn.MSELoss(),\n",
        "t_u_train = train_t_un,\n",
        "t_u_val = val_t_un,\n",
        "t_c_train = train_t_c,\n",
        "t_c_val = val_t_c)\n",
        "seq_model.parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6w7eBMb9AlY",
        "outputId": "57803562-428e-46b7-e605-612861a548c7"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 0.3328, Validation loss 0.2946\n",
            "Epoch 10, Training loss 0.3316, Validation loss 0.2935\n",
            "Epoch 20, Training loss 0.3279, Validation loss 0.2899\n",
            "Epoch 30, Training loss 0.3217, Validation loss 0.2840\n",
            "Epoch 40, Training loss 0.3130, Validation loss 0.2758\n",
            "Epoch 50, Training loss 0.3021, Validation loss 0.2655\n",
            "Epoch 60, Training loss 0.2892, Validation loss 0.2533\n",
            "Epoch 70, Training loss 0.2743, Validation loss 0.2393\n",
            "Epoch 80, Training loss 0.2579, Validation loss 0.2239\n",
            "Epoch 90, Training loss 0.2402, Validation loss 0.2072\n",
            "Epoch 100, Training loss 0.2214, Validation loss 0.1897\n",
            "Epoch 110, Training loss 0.2019, Validation loss 0.1715\n",
            "Epoch 120, Training loss 0.1820, Validation loss 0.1530\n",
            "Epoch 130, Training loss 0.1620, Validation loss 0.1346\n",
            "Epoch 140, Training loss 0.1423, Validation loss 0.1164\n",
            "Epoch 150, Training loss 0.1232, Validation loss 0.0990\n",
            "Epoch 160, Training loss 0.1049, Validation loss 0.0825\n",
            "Epoch 170, Training loss 0.0878, Validation loss 0.0673\n",
            "Epoch 180, Training loss 0.0722, Validation loss 0.0535\n",
            "Epoch 190, Training loss 0.0583, Validation loss 0.0416\n",
            "Epoch 200, Training loss 0.0464, Validation loss 0.0316\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of Sequential(\n",
              "  (hidden_linear): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (hidden_activation): Tanh()\n",
              "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "seq_model4 = nn.Sequential(OrderedDict([\n",
        "('hidden_linear', nn.Linear(5, 8)),\n",
        "('hidden_activation', nn.Tanh()),\n",
        "('hidden_linear2', nn.Linear(8, 16)),\n",
        "('hidden_activation2', nn.Tanh()),\n",
        "('hidden_linear3', nn.Linear(16, 64)),\n",
        "('hidden_activation3', nn.Tanh()),\n",
        "('hidden_linear4', nn.Linear(64, 16)),\n",
        "('hidden_activation4', nn.Tanh()),\n",
        "('output_linear', nn.Linear(16, 1))\n",
        "]))\n",
        "\n",
        "optimizer = optim.SGD(seq_model4.parameters(), lr=1e-5)\n",
        "\n",
        "training_loop(\n",
        "n_epochs = 200,\n",
        "optimizer = optimizer,\n",
        "model = seq_model4,\n",
        "loss_fn = nn.MSELoss(),\n",
        "t_u_train = train_t_un,\n",
        "t_u_val = val_t_un,\n",
        "t_c_train = train_t_c,\n",
        "t_c_val = val_t_c)\n",
        "seq_model.parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FP9sDjg49To3",
        "outputId": "c989a44b-fb43-4fba-f31c-d59a8192db94"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 0.1336, Validation loss 0.1084\n",
            "Epoch 10, Training loss 0.1330, Validation loss 0.1079\n",
            "Epoch 20, Training loss 0.1311, Validation loss 0.1061\n",
            "Epoch 30, Training loss 0.1279, Validation loss 0.1032\n",
            "Epoch 40, Training loss 0.1235, Validation loss 0.0992\n",
            "Epoch 50, Training loss 0.1180, Validation loss 0.0943\n",
            "Epoch 60, Training loss 0.1116, Validation loss 0.0884\n",
            "Epoch 70, Training loss 0.1043, Validation loss 0.0819\n",
            "Epoch 80, Training loss 0.0965, Validation loss 0.0749\n",
            "Epoch 90, Training loss 0.0882, Validation loss 0.0675\n",
            "Epoch 100, Training loss 0.0796, Validation loss 0.0600\n",
            "Epoch 110, Training loss 0.0710, Validation loss 0.0524\n",
            "Epoch 120, Training loss 0.0626, Validation loss 0.0452\n",
            "Epoch 130, Training loss 0.0545, Validation loss 0.0383\n",
            "Epoch 140, Training loss 0.0470, Validation loss 0.0321\n",
            "Epoch 150, Training loss 0.0402, Validation loss 0.0266\n",
            "Epoch 160, Training loss 0.0342, Validation loss 0.0220\n",
            "Epoch 170, Training loss 0.0292, Validation loss 0.0185\n",
            "Epoch 180, Training loss 0.0254, Validation loss 0.0161\n",
            "Epoch 190, Training loss 0.0227, Validation loss 0.0149\n",
            "Epoch 200, Training loss 0.0213, Validation loss 0.0150\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of Sequential(\n",
              "  (hidden_linear): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (hidden_activation): Tanh()\n",
              "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_model4(val_t_un)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvozswSGrDDp",
        "outputId": "7a39f020-1ab2-4c99-ee41-38d80793e458"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3512],\n",
              "        [0.3501],\n",
              "        [0.3509],\n",
              "        [0.3507],\n",
              "        [0.3514],\n",
              "        [0.3517],\n",
              "        [0.3502],\n",
              "        [0.3511],\n",
              "        [0.3506],\n",
              "        [0.3517],\n",
              "        [0.3498],\n",
              "        [0.3514],\n",
              "        [0.3499],\n",
              "        [0.3515],\n",
              "        [0.3503],\n",
              "        [0.3501],\n",
              "        [0.3509],\n",
              "        [0.3509],\n",
              "        [0.3512],\n",
              "        [0.3509],\n",
              "        [0.3510],\n",
              "        [0.3500],\n",
              "        [0.3506],\n",
              "        [0.3513],\n",
              "        [0.3511],\n",
              "        [0.3506],\n",
              "        [0.3480],\n",
              "        [0.3517],\n",
              "        [0.3515],\n",
              "        [0.3516],\n",
              "        [0.3510],\n",
              "        [0.3484],\n",
              "        [0.3510],\n",
              "        [0.3509],\n",
              "        [0.3517],\n",
              "        [0.3511],\n",
              "        [0.3517],\n",
              "        [0.3513],\n",
              "        [0.3511],\n",
              "        [0.3515],\n",
              "        [0.3498],\n",
              "        [0.3507],\n",
              "        [0.3510],\n",
              "        [0.3516],\n",
              "        [0.3490],\n",
              "        [0.3508],\n",
              "        [0.3502],\n",
              "        [0.3511],\n",
              "        [0.3500],\n",
              "        [0.3516],\n",
              "        [0.3498],\n",
              "        [0.3503],\n",
              "        [0.3515],\n",
              "        [0.3509],\n",
              "        [0.3512],\n",
              "        [0.3503],\n",
              "        [0.3516],\n",
              "        [0.3512],\n",
              "        [0.3509],\n",
              "        [0.3502],\n",
              "        [0.3507],\n",
              "        [0.3511],\n",
              "        [0.3509],\n",
              "        [0.3517],\n",
              "        [0.3496],\n",
              "        [0.3512],\n",
              "        [0.3491],\n",
              "        [0.3513],\n",
              "        [0.3500],\n",
              "        [0.3511],\n",
              "        [0.3497],\n",
              "        [0.3502],\n",
              "        [0.3514],\n",
              "        [0.3494],\n",
              "        [0.3505],\n",
              "        [0.3515],\n",
              "        [0.3506],\n",
              "        [0.3512],\n",
              "        [0.3503],\n",
              "        [0.3508],\n",
              "        [0.3506],\n",
              "        [0.3512],\n",
              "        [0.3512],\n",
              "        [0.3493],\n",
              "        [0.3502],\n",
              "        [0.3513],\n",
              "        [0.3510],\n",
              "        [0.3512],\n",
              "        [0.3512],\n",
              "        [0.3488],\n",
              "        [0.3505],\n",
              "        [0.3499],\n",
              "        [0.3491],\n",
              "        [0.3517],\n",
              "        [0.3513],\n",
              "        [0.3517],\n",
              "        [0.3511],\n",
              "        [0.3496],\n",
              "        [0.3501],\n",
              "        [0.3518],\n",
              "        [0.3517],\n",
              "        [0.3504],\n",
              "        [0.3513],\n",
              "        [0.3513],\n",
              "        [0.3493],\n",
              "        [0.3506],\n",
              "        [0.3517],\n",
              "        [0.3506],\n",
              "        [0.3517]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tun = 0.1 * tu\n",
        "\n",
        "torch.Tensor.ndim = property(lambda self: len(self.shape)) \n",
        "t_range = torch.arange(0., val_t_un.size(0)).unsqueeze(1)\n",
        "\n",
        "\n",
        "plt.xlabel(\"House Numbers\",color=\"black\")\n",
        "plt.ylabel(\"Actual House Prices\",color=\"black\")\n",
        "plt.plot(t_range.numpy(), seq_model(val_t_un).detach().numpy(),'r-')\n",
        "plt.plot(t_range.numpy(),val_t_c,'b-')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "5qKbFgiumzln",
        "outputId": "24fc517f-64e3-4f1a-bb9a-b4497451063c"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "_aWJIx1OKi9b"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "data_path = '../data-unversioned/p1ch7/'\n",
        "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)\n",
        "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "f686586819c44b09961a4d75b29fd515",
            "2e0427e52cd841bc8bbee207ae9963c4",
            "73d9d9eeb5384c96bc002cf9583f9544",
            "396018d11ac54bfe9bb85bcdeffc4a31",
            "8581677fe4954a48ab3b16a3c3946790",
            "69a80b8f4dcb4da4bdad2b84c5906e78",
            "76fcfd6cff38461dac1961606e512fc5",
            "7f0c210f698d49129dd6b70ddc7e5878",
            "7b0812ff6407413b869a25ff9f0277d6",
            "04c98f1bc1ff4a72b85a2b4a29e872c0",
            "5cd938bb1d0e42e1a1816b33ca526d9d"
          ]
        },
        "id": "YOPRmJfrKol6",
        "outputId": "51798048-5856-4613-b22b-8c50cd3cc203"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f686586819c44b09961a4d75b29fd515",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data-unversioned/p1ch7/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "to_tensor = transforms.ToTensor()\n",
        "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False,\n",
        "transform=transforms.ToTensor())"
      ],
      "metadata": {
        "id": "09HyEo7QRbRS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\n",
        "imgs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nJ5_5ubR2vo",
        "outputId": "92bb6384-37c0-4558-b98b-ba1e81971d30"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32, 50000])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_cifar10 = datasets.CIFAR10(\n",
        "data_path, train=True, download=False, transform=transforms.Compose([\n",
        "transforms.ToTensor(),\n",
        "transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "(0.2470, 0.2435, 0.2616))\n",
        "]))\n",
        "transformed_cifar10_val = datasets.CIFAR10(\n",
        "data_path, train=True, download=False, transform=transforms.Compose([\n",
        "transforms.ToTensor(),\n",
        "transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "(0.2470, 0.2435, 0.2616))\n",
        "]))"
      ],
      "metadata": {
        "id": "hi5q63S_SFgO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "            nn.Linear(3072, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.LogSoftmax(dim=1))"
      ],
      "metadata": {
        "id": "j414Y2v0K0Yo"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvD0I1u0aSnT",
        "outputId": "a2e910cd-0a3c-48c9-bbd1-052a4aa9d154"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dG5eEH7aZPn",
        "outputId": "7df1c2c5-e6c6-4da5-cde7-a19afc438c40"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=3072, out_features=512, bias=True)\n",
              "  (1): Tanh()\n",
              "  (2): Linear(in_features=512, out_features=10, bias=True)\n",
              "  (3): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ],
      "metadata": {
        "id": "OH8C5P3dtKP-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(transformed_cifar10, batch_size=64,\n",
        "shuffle=True)\n",
        "\n",
        "learning_rate = 1e-2\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "n_epochs = 200\n",
        "for epoch in range(n_epochs):\n",
        "  for img, label in train_loader:\n",
        "    img = img.to(device)\n",
        "    label = label.to(device)\n",
        "    out = model(img.view(img.shape[0], -1))\n",
        "    loss = loss_fn(out, label)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEUR0C20K8dD",
        "outputId": "36b3a137-50c7-4760-b23c-9b82e9923531"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 0.043088\n",
            "Epoch: 1, Loss: 0.057025\n",
            "Epoch: 2, Loss: 0.038201\n",
            "Epoch: 3, Loss: 0.047397\n",
            "Epoch: 4, Loss: 0.050660\n",
            "Epoch: 5, Loss: 0.059331\n",
            "Epoch: 6, Loss: 0.035562\n",
            "Epoch: 7, Loss: 0.036683\n",
            "Epoch: 8, Loss: 0.041616\n",
            "Epoch: 9, Loss: 0.035705\n",
            "Epoch: 10, Loss: 0.027001\n",
            "Epoch: 11, Loss: 0.028104\n",
            "Epoch: 12, Loss: 0.079878\n",
            "Epoch: 13, Loss: 0.042786\n",
            "Epoch: 14, Loss: 0.021934\n",
            "Epoch: 15, Loss: 0.026034\n",
            "Epoch: 16, Loss: 0.029837\n",
            "Epoch: 17, Loss: 0.022642\n",
            "Epoch: 18, Loss: 0.021015\n",
            "Epoch: 19, Loss: 0.026535\n",
            "Epoch: 20, Loss: 0.023743\n",
            "Epoch: 21, Loss: 0.031383\n",
            "Epoch: 22, Loss: 0.022712\n",
            "Epoch: 23, Loss: 0.041799\n",
            "Epoch: 24, Loss: 0.018079\n",
            "Epoch: 25, Loss: 0.027893\n",
            "Epoch: 26, Loss: 0.017242\n",
            "Epoch: 27, Loss: 0.044183\n",
            "Epoch: 28, Loss: 0.020814\n",
            "Epoch: 29, Loss: 0.028215\n",
            "Epoch: 30, Loss: 0.019496\n",
            "Epoch: 31, Loss: 0.021320\n",
            "Epoch: 32, Loss: 0.035963\n",
            "Epoch: 33, Loss: 0.025894\n",
            "Epoch: 34, Loss: 0.018425\n",
            "Epoch: 35, Loss: 0.019623\n",
            "Epoch: 36, Loss: 0.023952\n",
            "Epoch: 37, Loss: 0.021333\n",
            "Epoch: 38, Loss: 0.045734\n",
            "Epoch: 39, Loss: 0.032535\n",
            "Epoch: 40, Loss: 0.020625\n",
            "Epoch: 41, Loss: 0.019644\n",
            "Epoch: 42, Loss: 0.020146\n",
            "Epoch: 43, Loss: 0.021204\n",
            "Epoch: 44, Loss: 0.018879\n",
            "Epoch: 45, Loss: 0.015003\n",
            "Epoch: 46, Loss: 0.018539\n",
            "Epoch: 47, Loss: 0.019161\n",
            "Epoch: 48, Loss: 0.012766\n",
            "Epoch: 49, Loss: 0.024971\n",
            "Epoch: 50, Loss: 0.022887\n",
            "Epoch: 51, Loss: 0.020395\n",
            "Epoch: 52, Loss: 0.015677\n",
            "Epoch: 53, Loss: 0.020536\n",
            "Epoch: 54, Loss: 0.016874\n",
            "Epoch: 55, Loss: 0.019138\n",
            "Epoch: 56, Loss: 0.033469\n",
            "Epoch: 57, Loss: 0.019508\n",
            "Epoch: 58, Loss: 0.017040\n",
            "Epoch: 59, Loss: 0.023722\n",
            "Epoch: 60, Loss: 0.016914\n",
            "Epoch: 61, Loss: 0.023814\n",
            "Epoch: 62, Loss: 0.018294\n",
            "Epoch: 63, Loss: 0.024406\n",
            "Epoch: 64, Loss: 0.024359\n",
            "Epoch: 65, Loss: 0.027734\n",
            "Epoch: 66, Loss: 0.020873\n",
            "Epoch: 67, Loss: 0.018322\n",
            "Epoch: 68, Loss: 0.022165\n",
            "Epoch: 69, Loss: 0.015887\n",
            "Epoch: 70, Loss: 0.013098\n",
            "Epoch: 71, Loss: 0.019734\n",
            "Epoch: 72, Loss: 0.012129\n",
            "Epoch: 73, Loss: 0.014427\n",
            "Epoch: 74, Loss: 0.010175\n",
            "Epoch: 75, Loss: 0.013224\n",
            "Epoch: 76, Loss: 0.012405\n",
            "Epoch: 77, Loss: 0.005357\n",
            "Epoch: 78, Loss: 0.015773\n",
            "Epoch: 79, Loss: 0.008956\n",
            "Epoch: 80, Loss: 0.019661\n",
            "Epoch: 81, Loss: 0.015440\n",
            "Epoch: 82, Loss: 0.008173\n",
            "Epoch: 83, Loss: 0.007432\n",
            "Epoch: 84, Loss: 0.011209\n",
            "Epoch: 85, Loss: 0.018338\n",
            "Epoch: 86, Loss: 0.012831\n",
            "Epoch: 87, Loss: 0.011702\n",
            "Epoch: 88, Loss: 0.014756\n",
            "Epoch: 89, Loss: 0.011848\n",
            "Epoch: 90, Loss: 0.018560\n",
            "Epoch: 91, Loss: 0.008299\n",
            "Epoch: 92, Loss: 0.011431\n",
            "Epoch: 93, Loss: 0.005641\n",
            "Epoch: 94, Loss: 0.014997\n",
            "Epoch: 95, Loss: 0.006786\n",
            "Epoch: 96, Loss: 0.007444\n",
            "Epoch: 97, Loss: 0.013103\n",
            "Epoch: 98, Loss: 0.008859\n",
            "Epoch: 99, Loss: 0.010383\n",
            "Epoch: 100, Loss: 0.013520\n",
            "Epoch: 101, Loss: 0.009444\n",
            "Epoch: 102, Loss: 0.009269\n",
            "Epoch: 103, Loss: 0.016714\n",
            "Epoch: 104, Loss: 0.008875\n",
            "Epoch: 105, Loss: 0.015531\n",
            "Epoch: 106, Loss: 0.005297\n",
            "Epoch: 107, Loss: 0.009350\n",
            "Epoch: 108, Loss: 0.011260\n",
            "Epoch: 109, Loss: 0.013332\n",
            "Epoch: 110, Loss: 0.009264\n",
            "Epoch: 111, Loss: 0.010810\n",
            "Epoch: 112, Loss: 0.009677\n",
            "Epoch: 113, Loss: 0.010775\n",
            "Epoch: 114, Loss: 0.009703\n",
            "Epoch: 115, Loss: 0.011538\n",
            "Epoch: 116, Loss: 0.011116\n",
            "Epoch: 117, Loss: 0.007395\n",
            "Epoch: 118, Loss: 0.011066\n",
            "Epoch: 119, Loss: 0.015425\n",
            "Epoch: 120, Loss: 0.012492\n",
            "Epoch: 121, Loss: 0.008298\n",
            "Epoch: 122, Loss: 0.009747\n",
            "Epoch: 123, Loss: 0.006837\n",
            "Epoch: 124, Loss: 0.007590\n",
            "Epoch: 125, Loss: 0.008321\n",
            "Epoch: 126, Loss: 0.009450\n",
            "Epoch: 127, Loss: 0.008754\n",
            "Epoch: 128, Loss: 0.006749\n",
            "Epoch: 129, Loss: 0.007135\n",
            "Epoch: 130, Loss: 0.006987\n",
            "Epoch: 131, Loss: 0.009399\n",
            "Epoch: 132, Loss: 0.011227\n",
            "Epoch: 133, Loss: 0.012594\n",
            "Epoch: 134, Loss: 0.010278\n",
            "Epoch: 135, Loss: 0.007448\n",
            "Epoch: 136, Loss: 0.010064\n",
            "Epoch: 137, Loss: 0.008731\n",
            "Epoch: 138, Loss: 0.005083\n",
            "Epoch: 139, Loss: 0.006815\n",
            "Epoch: 140, Loss: 0.006416\n",
            "Epoch: 141, Loss: 0.009107\n",
            "Epoch: 142, Loss: 0.012667\n",
            "Epoch: 143, Loss: 0.007035\n",
            "Epoch: 144, Loss: 0.008153\n",
            "Epoch: 145, Loss: 0.009860\n",
            "Epoch: 146, Loss: 0.006620\n",
            "Epoch: 147, Loss: 0.008852\n",
            "Epoch: 148, Loss: 0.004814\n",
            "Epoch: 149, Loss: 0.010056\n",
            "Epoch: 150, Loss: 0.005906\n",
            "Epoch: 151, Loss: 0.009064\n",
            "Epoch: 152, Loss: 0.011506\n",
            "Epoch: 153, Loss: 0.007883\n",
            "Epoch: 154, Loss: 0.009137\n",
            "Epoch: 155, Loss: 0.008435\n",
            "Epoch: 156, Loss: 0.008699\n",
            "Epoch: 157, Loss: 0.006636\n",
            "Epoch: 158, Loss: 0.007252\n",
            "Epoch: 159, Loss: 0.005915\n",
            "Epoch: 160, Loss: 0.005877\n",
            "Epoch: 161, Loss: 0.005368\n",
            "Epoch: 162, Loss: 0.008521\n",
            "Epoch: 163, Loss: 0.008411\n",
            "Epoch: 164, Loss: 0.006963\n",
            "Epoch: 165, Loss: 0.008592\n",
            "Epoch: 166, Loss: 0.007188\n",
            "Epoch: 167, Loss: 0.007325\n",
            "Epoch: 168, Loss: 0.005458\n",
            "Epoch: 169, Loss: 0.006199\n",
            "Epoch: 170, Loss: 0.007820\n",
            "Epoch: 171, Loss: 0.008170\n",
            "Epoch: 172, Loss: 0.004782\n",
            "Epoch: 173, Loss: 0.005103\n",
            "Epoch: 174, Loss: 0.007434\n",
            "Epoch: 175, Loss: 0.004662\n",
            "Epoch: 176, Loss: 0.004661\n",
            "Epoch: 177, Loss: 0.009087\n",
            "Epoch: 178, Loss: 0.009862\n",
            "Epoch: 179, Loss: 0.005682\n",
            "Epoch: 180, Loss: 0.003891\n",
            "Epoch: 181, Loss: 0.004639\n",
            "Epoch: 182, Loss: 0.007159\n",
            "Epoch: 183, Loss: 0.007528\n",
            "Epoch: 184, Loss: 0.006132\n",
            "Epoch: 185, Loss: 0.011811\n",
            "Epoch: 186, Loss: 0.008165\n",
            "Epoch: 187, Loss: 0.005663\n",
            "Epoch: 188, Loss: 0.007004\n",
            "Epoch: 189, Loss: 0.005210\n",
            "Epoch: 190, Loss: 0.006422\n",
            "Epoch: 191, Loss: 0.005012\n",
            "Epoch: 192, Loss: 0.005770\n",
            "Epoch: 193, Loss: 0.008819\n",
            "Epoch: 194, Loss: 0.008264\n",
            "Epoch: 195, Loss: 0.005651\n",
            "Epoch: 196, Loss: 0.004982\n",
            "Epoch: 197, Loss: 0.007130\n",
            "Epoch: 198, Loss: 0.004959\n",
            "Epoch: 199, Loss: 0.010854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = torch.utils.data.DataLoader(transformed_cifar10_val, batch_size=64,\n",
        "shuffle=False)\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for img, label in val_loader:\n",
        "    img = img.to(device)\n",
        "    label = label.to(device)\n",
        "    batch_size = img.shape[0]\n",
        "    outputs = model(img.view(batch_size, -1))\n",
        "    _, predicted = torch.max(outputs, dim=1)\n",
        "    total += label.shape[0]\n",
        "    correct += int((predicted == label).sum())\n",
        "print(\"Accuracy: %f\", correct / total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGs-QWr9ozyj",
        "outputId": "06bbc1f3-6b81-4ff2-9dcc-3a0600bb39f4"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: %f 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "            nn.Linear(3072, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.LogSoftmax(dim=1))\n",
        "model.to(device)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(transformed_cifar10, batch_size=64,\n",
        "shuffle=True)\n",
        "\n",
        "learning_rate = 1e-2\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "n_epochs = 300\n",
        "for epoch in range(n_epochs):\n",
        "  for img, label in train_loader:\n",
        "    img = img.to(device)\n",
        "    label = label.to(device)\n",
        "    out = model(img.view(img.shape[0], -1))\n",
        "    loss = loss_fn(out, label)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGfqd9LC4HvN",
        "outputId": "36dd6b3d-6b31-4f7a-9160-86af12a6222e"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 1.948675\n",
            "Epoch: 1, Loss: 1.656898\n",
            "Epoch: 2, Loss: 1.458983\n",
            "Epoch: 3, Loss: 1.630197\n",
            "Epoch: 4, Loss: 1.533011\n",
            "Epoch: 5, Loss: 1.737972\n",
            "Epoch: 6, Loss: 1.981542\n",
            "Epoch: 7, Loss: 1.593217\n",
            "Epoch: 8, Loss: 1.479205\n",
            "Epoch: 9, Loss: 1.329820\n",
            "Epoch: 10, Loss: 1.424241\n",
            "Epoch: 11, Loss: 1.357489\n",
            "Epoch: 12, Loss: 1.476130\n",
            "Epoch: 13, Loss: 1.053941\n",
            "Epoch: 14, Loss: 1.024684\n",
            "Epoch: 15, Loss: 0.817721\n",
            "Epoch: 16, Loss: 1.430803\n",
            "Epoch: 17, Loss: 1.068134\n",
            "Epoch: 18, Loss: 0.709538\n",
            "Epoch: 19, Loss: 1.286681\n",
            "Epoch: 20, Loss: 1.167257\n",
            "Epoch: 21, Loss: 0.621951\n",
            "Epoch: 22, Loss: 0.864668\n",
            "Epoch: 23, Loss: 1.183190\n",
            "Epoch: 24, Loss: 0.841900\n",
            "Epoch: 25, Loss: 1.090573\n",
            "Epoch: 26, Loss: 0.907429\n",
            "Epoch: 27, Loss: 0.732233\n",
            "Epoch: 28, Loss: 0.522543\n",
            "Epoch: 29, Loss: 0.559086\n",
            "Epoch: 30, Loss: 0.705893\n",
            "Epoch: 31, Loss: 0.661692\n",
            "Epoch: 32, Loss: 0.675343\n",
            "Epoch: 33, Loss: 0.562876\n",
            "Epoch: 34, Loss: 0.560110\n",
            "Epoch: 35, Loss: 0.452437\n",
            "Epoch: 36, Loss: 0.768383\n",
            "Epoch: 37, Loss: 0.531435\n",
            "Epoch: 38, Loss: 0.910458\n",
            "Epoch: 39, Loss: 0.464963\n",
            "Epoch: 40, Loss: 0.639977\n",
            "Epoch: 41, Loss: 0.248142\n",
            "Epoch: 42, Loss: 0.334359\n",
            "Epoch: 43, Loss: 0.403610\n",
            "Epoch: 44, Loss: 0.505605\n",
            "Epoch: 45, Loss: 0.304269\n",
            "Epoch: 46, Loss: 0.390309\n",
            "Epoch: 47, Loss: 0.410829\n",
            "Epoch: 48, Loss: 0.586535\n",
            "Epoch: 49, Loss: 0.388419\n",
            "Epoch: 50, Loss: 0.211681\n",
            "Epoch: 51, Loss: 0.322202\n",
            "Epoch: 52, Loss: 0.295017\n",
            "Epoch: 53, Loss: 0.308769\n",
            "Epoch: 54, Loss: 0.260576\n",
            "Epoch: 55, Loss: 0.219844\n",
            "Epoch: 56, Loss: 0.156236\n",
            "Epoch: 57, Loss: 0.278026\n",
            "Epoch: 58, Loss: 0.489493\n",
            "Epoch: 59, Loss: 0.091578\n",
            "Epoch: 60, Loss: 0.293267\n",
            "Epoch: 61, Loss: 0.176632\n",
            "Epoch: 62, Loss: 0.244469\n",
            "Epoch: 63, Loss: 0.195570\n",
            "Epoch: 64, Loss: 0.121758\n",
            "Epoch: 65, Loss: 0.071098\n",
            "Epoch: 66, Loss: 0.093924\n",
            "Epoch: 67, Loss: 0.110092\n",
            "Epoch: 68, Loss: 0.130987\n",
            "Epoch: 69, Loss: 0.118581\n",
            "Epoch: 70, Loss: 0.103258\n",
            "Epoch: 71, Loss: 0.097287\n",
            "Epoch: 72, Loss: 0.203630\n",
            "Epoch: 73, Loss: 0.094365\n",
            "Epoch: 74, Loss: 0.117813\n",
            "Epoch: 75, Loss: 0.043336\n",
            "Epoch: 76, Loss: 0.047895\n",
            "Epoch: 77, Loss: 0.093697\n",
            "Epoch: 78, Loss: 0.085295\n",
            "Epoch: 79, Loss: 0.085383\n",
            "Epoch: 80, Loss: 0.089089\n",
            "Epoch: 81, Loss: 0.066351\n",
            "Epoch: 82, Loss: 0.053687\n",
            "Epoch: 83, Loss: 0.086611\n",
            "Epoch: 84, Loss: 0.056016\n",
            "Epoch: 85, Loss: 0.048740\n",
            "Epoch: 86, Loss: 0.041658\n",
            "Epoch: 87, Loss: 0.054476\n",
            "Epoch: 88, Loss: 0.038773\n",
            "Epoch: 89, Loss: 0.049023\n",
            "Epoch: 90, Loss: 0.069379\n",
            "Epoch: 91, Loss: 0.119249\n",
            "Epoch: 92, Loss: 0.038001\n",
            "Epoch: 93, Loss: 0.193991\n",
            "Epoch: 94, Loss: 0.055129\n",
            "Epoch: 95, Loss: 0.036308\n",
            "Epoch: 96, Loss: 0.032966\n",
            "Epoch: 97, Loss: 0.036585\n",
            "Epoch: 98, Loss: 0.065460\n",
            "Epoch: 99, Loss: 0.023560\n",
            "Epoch: 100, Loss: 0.025731\n",
            "Epoch: 101, Loss: 0.043009\n",
            "Epoch: 102, Loss: 0.056507\n",
            "Epoch: 103, Loss: 0.030408\n",
            "Epoch: 104, Loss: 0.046037\n",
            "Epoch: 105, Loss: 0.022946\n",
            "Epoch: 106, Loss: 0.032346\n",
            "Epoch: 107, Loss: 0.041426\n",
            "Epoch: 108, Loss: 0.068171\n",
            "Epoch: 109, Loss: 0.036676\n",
            "Epoch: 110, Loss: 0.023060\n",
            "Epoch: 111, Loss: 0.039028\n",
            "Epoch: 112, Loss: 0.053686\n",
            "Epoch: 113, Loss: 0.032468\n",
            "Epoch: 114, Loss: 0.041395\n",
            "Epoch: 115, Loss: 0.014925\n",
            "Epoch: 116, Loss: 0.039563\n",
            "Epoch: 117, Loss: 0.026412\n",
            "Epoch: 118, Loss: 0.027982\n",
            "Epoch: 119, Loss: 0.024671\n",
            "Epoch: 120, Loss: 0.028310\n",
            "Epoch: 121, Loss: 0.020873\n",
            "Epoch: 122, Loss: 0.074838\n",
            "Epoch: 123, Loss: 0.013736\n",
            "Epoch: 124, Loss: 0.031247\n",
            "Epoch: 125, Loss: 0.019837\n",
            "Epoch: 126, Loss: 0.015826\n",
            "Epoch: 127, Loss: 0.036868\n",
            "Epoch: 128, Loss: 0.020476\n",
            "Epoch: 129, Loss: 0.030869\n",
            "Epoch: 130, Loss: 0.011393\n",
            "Epoch: 131, Loss: 0.023355\n",
            "Epoch: 132, Loss: 0.033752\n",
            "Epoch: 133, Loss: 0.029432\n",
            "Epoch: 134, Loss: 0.014024\n",
            "Epoch: 135, Loss: 0.025310\n",
            "Epoch: 136, Loss: 0.016060\n",
            "Epoch: 137, Loss: 0.023410\n",
            "Epoch: 138, Loss: 0.026093\n",
            "Epoch: 139, Loss: 0.019195\n",
            "Epoch: 140, Loss: 0.020264\n",
            "Epoch: 141, Loss: 0.017152\n",
            "Epoch: 142, Loss: 0.023843\n",
            "Epoch: 143, Loss: 0.015049\n",
            "Epoch: 144, Loss: 0.018138\n",
            "Epoch: 145, Loss: 0.014514\n",
            "Epoch: 146, Loss: 0.031552\n",
            "Epoch: 147, Loss: 0.023299\n",
            "Epoch: 148, Loss: 0.021255\n",
            "Epoch: 149, Loss: 0.018564\n",
            "Epoch: 150, Loss: 0.106029\n",
            "Epoch: 151, Loss: 0.022386\n",
            "Epoch: 152, Loss: 0.016713\n",
            "Epoch: 153, Loss: 0.018397\n",
            "Epoch: 154, Loss: 0.014424\n",
            "Epoch: 155, Loss: 0.037627\n",
            "Epoch: 156, Loss: 0.009997\n",
            "Epoch: 157, Loss: 0.012851\n",
            "Epoch: 158, Loss: 0.015676\n",
            "Epoch: 159, Loss: 0.018366\n",
            "Epoch: 160, Loss: 0.015793\n",
            "Epoch: 161, Loss: 0.017601\n",
            "Epoch: 162, Loss: 0.012799\n",
            "Epoch: 163, Loss: 0.014239\n",
            "Epoch: 164, Loss: 0.020221\n",
            "Epoch: 165, Loss: 0.008720\n",
            "Epoch: 166, Loss: 0.009040\n",
            "Epoch: 167, Loss: 0.015298\n",
            "Epoch: 168, Loss: 0.013887\n",
            "Epoch: 169, Loss: 0.015480\n",
            "Epoch: 170, Loss: 0.011705\n",
            "Epoch: 171, Loss: 0.011296\n",
            "Epoch: 172, Loss: 0.013466\n",
            "Epoch: 173, Loss: 0.011223\n",
            "Epoch: 174, Loss: 0.010732\n",
            "Epoch: 175, Loss: 0.020704\n",
            "Epoch: 176, Loss: 0.011626\n",
            "Epoch: 177, Loss: 0.008223\n",
            "Epoch: 178, Loss: 0.016068\n",
            "Epoch: 179, Loss: 0.012103\n",
            "Epoch: 180, Loss: 0.006105\n",
            "Epoch: 181, Loss: 0.012968\n",
            "Epoch: 182, Loss: 0.020006\n",
            "Epoch: 183, Loss: 0.016023\n",
            "Epoch: 184, Loss: 0.010093\n",
            "Epoch: 185, Loss: 0.009748\n",
            "Epoch: 186, Loss: 0.016483\n",
            "Epoch: 187, Loss: 0.011807\n",
            "Epoch: 188, Loss: 0.007053\n",
            "Epoch: 189, Loss: 0.014511\n",
            "Epoch: 190, Loss: 0.008449\n",
            "Epoch: 191, Loss: 0.014046\n",
            "Epoch: 192, Loss: 0.014516\n",
            "Epoch: 193, Loss: 0.011570\n",
            "Epoch: 194, Loss: 0.011639\n",
            "Epoch: 195, Loss: 0.008677\n",
            "Epoch: 196, Loss: 0.010794\n",
            "Epoch: 197, Loss: 0.010843\n",
            "Epoch: 198, Loss: 0.009047\n",
            "Epoch: 199, Loss: 0.010883\n",
            "Epoch: 200, Loss: 0.014259\n",
            "Epoch: 201, Loss: 0.006375\n",
            "Epoch: 202, Loss: 0.016819\n",
            "Epoch: 203, Loss: 0.010274\n",
            "Epoch: 204, Loss: 0.008771\n",
            "Epoch: 205, Loss: 0.008823\n",
            "Epoch: 206, Loss: 0.008916\n",
            "Epoch: 207, Loss: 0.013107\n",
            "Epoch: 208, Loss: 0.008770\n",
            "Epoch: 209, Loss: 0.011585\n",
            "Epoch: 210, Loss: 0.012021\n",
            "Epoch: 211, Loss: 0.006232\n",
            "Epoch: 212, Loss: 0.017153\n",
            "Epoch: 213, Loss: 0.012333\n",
            "Epoch: 214, Loss: 0.009845\n",
            "Epoch: 215, Loss: 0.010894\n",
            "Epoch: 216, Loss: 0.006212\n",
            "Epoch: 217, Loss: 0.008933\n",
            "Epoch: 218, Loss: 0.008486\n",
            "Epoch: 219, Loss: 0.009581\n",
            "Epoch: 220, Loss: 0.008160\n",
            "Epoch: 221, Loss: 0.007405\n",
            "Epoch: 222, Loss: 0.007757\n",
            "Epoch: 223, Loss: 0.005355\n",
            "Epoch: 224, Loss: 0.007017\n",
            "Epoch: 225, Loss: 0.010243\n",
            "Epoch: 226, Loss: 0.006127\n",
            "Epoch: 227, Loss: 0.006895\n",
            "Epoch: 228, Loss: 0.006583\n",
            "Epoch: 229, Loss: 0.007958\n",
            "Epoch: 230, Loss: 0.007333\n",
            "Epoch: 231, Loss: 0.009865\n",
            "Epoch: 232, Loss: 0.008271\n",
            "Epoch: 233, Loss: 0.009954\n",
            "Epoch: 234, Loss: 0.008696\n",
            "Epoch: 235, Loss: 0.008576\n",
            "Epoch: 236, Loss: 0.011787\n",
            "Epoch: 237, Loss: 0.007608\n",
            "Epoch: 238, Loss: 0.008418\n",
            "Epoch: 239, Loss: 0.006930\n",
            "Epoch: 240, Loss: 0.007504\n",
            "Epoch: 241, Loss: 0.008145\n",
            "Epoch: 242, Loss: 0.008896\n",
            "Epoch: 243, Loss: 0.009124\n",
            "Epoch: 244, Loss: 0.010860\n",
            "Epoch: 245, Loss: 0.012078\n",
            "Epoch: 246, Loss: 0.005710\n",
            "Epoch: 247, Loss: 0.009471\n",
            "Epoch: 248, Loss: 0.008826\n",
            "Epoch: 249, Loss: 0.005934\n",
            "Epoch: 250, Loss: 0.007819\n",
            "Epoch: 251, Loss: 0.008163\n",
            "Epoch: 252, Loss: 0.014136\n",
            "Epoch: 253, Loss: 0.007697\n",
            "Epoch: 254, Loss: 0.007476\n",
            "Epoch: 255, Loss: 0.005776\n",
            "Epoch: 256, Loss: 0.008691\n",
            "Epoch: 257, Loss: 0.008056\n",
            "Epoch: 258, Loss: 0.006855\n",
            "Epoch: 259, Loss: 0.009676\n",
            "Epoch: 260, Loss: 0.005307\n",
            "Epoch: 261, Loss: 0.007023\n",
            "Epoch: 262, Loss: 0.006941\n",
            "Epoch: 263, Loss: 0.009497\n",
            "Epoch: 264, Loss: 0.007673\n",
            "Epoch: 265, Loss: 0.007725\n",
            "Epoch: 266, Loss: 0.005373\n",
            "Epoch: 267, Loss: 0.007070\n",
            "Epoch: 268, Loss: 0.009318\n",
            "Epoch: 269, Loss: 0.008133\n",
            "Epoch: 270, Loss: 0.005300\n",
            "Epoch: 271, Loss: 0.006121\n",
            "Epoch: 272, Loss: 0.006455\n",
            "Epoch: 273, Loss: 0.008630\n",
            "Epoch: 274, Loss: 0.006070\n",
            "Epoch: 275, Loss: 0.005128\n",
            "Epoch: 276, Loss: 0.007165\n",
            "Epoch: 277, Loss: 0.006622\n",
            "Epoch: 278, Loss: 0.007638\n",
            "Epoch: 279, Loss: 0.006704\n",
            "Epoch: 280, Loss: 0.008447\n",
            "Epoch: 281, Loss: 0.005915\n",
            "Epoch: 282, Loss: 0.007894\n",
            "Epoch: 283, Loss: 0.008250\n",
            "Epoch: 284, Loss: 0.005976\n",
            "Epoch: 285, Loss: 0.005111\n",
            "Epoch: 286, Loss: 0.007959\n",
            "Epoch: 287, Loss: 0.007723\n",
            "Epoch: 288, Loss: 0.006350\n",
            "Epoch: 289, Loss: 0.005229\n",
            "Epoch: 290, Loss: 0.004792\n",
            "Epoch: 291, Loss: 0.006341\n",
            "Epoch: 292, Loss: 0.004136\n",
            "Epoch: 293, Loss: 0.008181\n",
            "Epoch: 294, Loss: 0.004478\n",
            "Epoch: 295, Loss: 0.006199\n",
            "Epoch: 296, Loss: 0.007104\n",
            "Epoch: 297, Loss: 0.005249\n",
            "Epoch: 298, Loss: 0.005217\n",
            "Epoch: 299, Loss: 0.006299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = torch.utils.data.DataLoader(transformed_cifar10_val, batch_size=64,\n",
        "shuffle=False)\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for img, label in val_loader:\n",
        "    img = img.to(device)\n",
        "    label = label.to(device)\n",
        "    batch_size = img.shape[0]\n",
        "    outputs = model(img.view(batch_size, -1))\n",
        "    _, predicted = torch.max(outputs, dim=1)\n",
        "    total += label.shape[0]\n",
        "    correct += int((predicted == label).sum())\n",
        "print(\"Accuracy: %f\", correct / total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY6Iiyj54V4b",
        "outputId": "19a7a1af-8305-460b-cac6-58252e4acb06"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: %f 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "            nn.Linear(3072, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.LogSoftmax(dim=1))\n",
        "model.to(device)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(transformed_cifar10, batch_size=64,\n",
        "shuffle=True)\n",
        "\n",
        "learning_rate = 1e-2\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "n_epochs = 300\n",
        "for epoch in range(n_epochs):\n",
        "  for img, label in train_loader:\n",
        "    img = img.to(device)\n",
        "    label = label.to(device)\n",
        "    out = model(img.view(img.shape[0], -1))\n",
        "    loss = loss_fn(out, label)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAf1w_MC7fQK",
        "outputId": "c1ad1874-3ef8-4c87-e0d4-1314730150c3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 1.931821\n",
            "Epoch: 1, Loss: 1.636644\n",
            "Epoch: 2, Loss: 1.636295\n",
            "Epoch: 3, Loss: 1.638169\n",
            "Epoch: 4, Loss: 1.487793\n",
            "Epoch: 5, Loss: 1.352309\n",
            "Epoch: 6, Loss: 1.389350\n",
            "Epoch: 7, Loss: 1.655078\n",
            "Epoch: 8, Loss: 1.486730\n",
            "Epoch: 9, Loss: 1.514642\n",
            "Epoch: 10, Loss: 1.067615\n",
            "Epoch: 11, Loss: 0.950669\n",
            "Epoch: 12, Loss: 1.518697\n",
            "Epoch: 13, Loss: 1.381049\n",
            "Epoch: 14, Loss: 1.354407\n",
            "Epoch: 15, Loss: 1.477752\n",
            "Epoch: 16, Loss: 1.059184\n",
            "Epoch: 17, Loss: 1.163421\n",
            "Epoch: 18, Loss: 0.627815\n",
            "Epoch: 19, Loss: 0.677120\n",
            "Epoch: 20, Loss: 0.979858\n",
            "Epoch: 21, Loss: 0.810467\n",
            "Epoch: 22, Loss: 0.383633\n",
            "Epoch: 23, Loss: 0.847527\n",
            "Epoch: 24, Loss: 0.489229\n",
            "Epoch: 25, Loss: 0.942383\n",
            "Epoch: 26, Loss: 0.868800\n",
            "Epoch: 27, Loss: 0.362631\n",
            "Epoch: 28, Loss: 0.697073\n",
            "Epoch: 29, Loss: 0.335870\n",
            "Epoch: 30, Loss: 0.808650\n",
            "Epoch: 31, Loss: 0.359018\n",
            "Epoch: 32, Loss: 1.146888\n",
            "Epoch: 33, Loss: 0.189084\n",
            "Epoch: 34, Loss: 0.145760\n",
            "Epoch: 35, Loss: 0.252126\n",
            "Epoch: 36, Loss: 0.368170\n",
            "Epoch: 37, Loss: 0.415067\n",
            "Epoch: 38, Loss: 0.131719\n",
            "Epoch: 39, Loss: 0.225043\n",
            "Epoch: 40, Loss: 0.130996\n",
            "Epoch: 41, Loss: 0.414136\n",
            "Epoch: 42, Loss: 0.097371\n",
            "Epoch: 43, Loss: 0.442982\n",
            "Epoch: 44, Loss: 0.134047\n",
            "Epoch: 45, Loss: 0.104675\n",
            "Epoch: 46, Loss: 0.480784\n",
            "Epoch: 47, Loss: 0.261426\n",
            "Epoch: 48, Loss: 0.186428\n",
            "Epoch: 49, Loss: 0.443434\n",
            "Epoch: 50, Loss: 0.116795\n",
            "Epoch: 51, Loss: 0.065785\n",
            "Epoch: 52, Loss: 0.254856\n",
            "Epoch: 53, Loss: 0.070858\n",
            "Epoch: 54, Loss: 0.045613\n",
            "Epoch: 55, Loss: 0.059988\n",
            "Epoch: 56, Loss: 0.062102\n",
            "Epoch: 57, Loss: 0.080472\n",
            "Epoch: 58, Loss: 0.013057\n",
            "Epoch: 59, Loss: 0.046126\n",
            "Epoch: 60, Loss: 0.079579\n",
            "Epoch: 61, Loss: 0.041594\n",
            "Epoch: 62, Loss: 0.018261\n",
            "Epoch: 63, Loss: 0.017554\n",
            "Epoch: 64, Loss: 0.025177\n",
            "Epoch: 65, Loss: 0.040453\n",
            "Epoch: 66, Loss: 0.022790\n",
            "Epoch: 67, Loss: 0.011266\n",
            "Epoch: 68, Loss: 0.014670\n",
            "Epoch: 69, Loss: 0.017553\n",
            "Epoch: 70, Loss: 0.010370\n",
            "Epoch: 71, Loss: 0.007213\n",
            "Epoch: 72, Loss: 0.005314\n",
            "Epoch: 73, Loss: 0.005734\n",
            "Epoch: 74, Loss: 0.007064\n",
            "Epoch: 75, Loss: 0.010976\n",
            "Epoch: 76, Loss: 0.004674\n",
            "Epoch: 77, Loss: 0.003426\n",
            "Epoch: 78, Loss: 0.012123\n",
            "Epoch: 79, Loss: 0.011704\n",
            "Epoch: 80, Loss: 0.003810\n",
            "Epoch: 81, Loss: 0.002339\n",
            "Epoch: 82, Loss: 1.376440\n",
            "Epoch: 83, Loss: 0.708612\n",
            "Epoch: 84, Loss: 0.110245\n",
            "Epoch: 85, Loss: 0.014237\n",
            "Epoch: 86, Loss: 0.006333\n",
            "Epoch: 87, Loss: 0.013548\n",
            "Epoch: 88, Loss: 0.003186\n",
            "Epoch: 89, Loss: 0.007118\n",
            "Epoch: 90, Loss: 0.004682\n",
            "Epoch: 91, Loss: 0.005942\n",
            "Epoch: 92, Loss: 0.003063\n",
            "Epoch: 93, Loss: 0.005659\n",
            "Epoch: 94, Loss: 0.003711\n",
            "Epoch: 95, Loss: 0.008757\n",
            "Epoch: 96, Loss: 0.004372\n",
            "Epoch: 97, Loss: 0.005566\n",
            "Epoch: 98, Loss: 0.003482\n",
            "Epoch: 99, Loss: 0.002898\n",
            "Epoch: 100, Loss: 0.001870\n",
            "Epoch: 101, Loss: 0.006642\n",
            "Epoch: 102, Loss: 0.005562\n",
            "Epoch: 103, Loss: 0.003806\n",
            "Epoch: 104, Loss: 0.004588\n",
            "Epoch: 105, Loss: 0.003442\n",
            "Epoch: 106, Loss: 0.003307\n",
            "Epoch: 107, Loss: 0.002969\n",
            "Epoch: 108, Loss: 0.003154\n",
            "Epoch: 109, Loss: 0.001524\n",
            "Epoch: 110, Loss: 0.001916\n",
            "Epoch: 111, Loss: 0.002164\n",
            "Epoch: 112, Loss: 0.001707\n",
            "Epoch: 113, Loss: 0.001480\n",
            "Epoch: 114, Loss: 0.002217\n",
            "Epoch: 115, Loss: 0.002811\n",
            "Epoch: 116, Loss: 0.003619\n",
            "Epoch: 117, Loss: 0.005993\n",
            "Epoch: 118, Loss: 0.001902\n",
            "Epoch: 119, Loss: 0.002827\n",
            "Epoch: 120, Loss: 0.004238\n",
            "Epoch: 121, Loss: 0.002310\n",
            "Epoch: 122, Loss: 0.002451\n",
            "Epoch: 123, Loss: 0.002136\n",
            "Epoch: 124, Loss: 0.002096\n",
            "Epoch: 125, Loss: 0.002205\n",
            "Epoch: 126, Loss: 0.002074\n",
            "Epoch: 127, Loss: 0.001373\n",
            "Epoch: 128, Loss: 0.003655\n",
            "Epoch: 129, Loss: 0.001412\n",
            "Epoch: 130, Loss: 0.001604\n",
            "Epoch: 131, Loss: 0.001490\n",
            "Epoch: 132, Loss: 0.001951\n",
            "Epoch: 133, Loss: 0.001979\n",
            "Epoch: 134, Loss: 0.003227\n",
            "Epoch: 135, Loss: 0.001681\n",
            "Epoch: 136, Loss: 0.002903\n",
            "Epoch: 137, Loss: 0.005599\n",
            "Epoch: 138, Loss: 0.002807\n",
            "Epoch: 139, Loss: 0.000487\n",
            "Epoch: 140, Loss: 0.001352\n",
            "Epoch: 141, Loss: 0.001555\n",
            "Epoch: 142, Loss: 0.001112\n",
            "Epoch: 143, Loss: 0.001939\n",
            "Epoch: 144, Loss: 0.002846\n",
            "Epoch: 145, Loss: 0.001774\n",
            "Epoch: 146, Loss: 0.002394\n",
            "Epoch: 147, Loss: 0.003193\n",
            "Epoch: 148, Loss: 0.001270\n",
            "Epoch: 149, Loss: 0.001691\n",
            "Epoch: 150, Loss: 0.000934\n",
            "Epoch: 151, Loss: 0.002290\n",
            "Epoch: 152, Loss: 0.002660\n",
            "Epoch: 153, Loss: 0.001107\n",
            "Epoch: 154, Loss: 0.003028\n",
            "Epoch: 155, Loss: 0.001863\n",
            "Epoch: 156, Loss: 0.001883\n",
            "Epoch: 157, Loss: 0.001417\n",
            "Epoch: 158, Loss: 0.001575\n",
            "Epoch: 159, Loss: 0.001737\n",
            "Epoch: 160, Loss: 0.001511\n",
            "Epoch: 161, Loss: 0.001341\n",
            "Epoch: 162, Loss: 0.001231\n",
            "Epoch: 163, Loss: 0.001136\n",
            "Epoch: 164, Loss: 0.002225\n",
            "Epoch: 165, Loss: 0.001381\n",
            "Epoch: 166, Loss: 0.000796\n",
            "Epoch: 167, Loss: 0.001426\n",
            "Epoch: 168, Loss: 0.001431\n",
            "Epoch: 169, Loss: 0.001671\n",
            "Epoch: 170, Loss: 0.000779\n",
            "Epoch: 171, Loss: 0.001051\n",
            "Epoch: 172, Loss: 0.001545\n",
            "Epoch: 173, Loss: 0.003697\n",
            "Epoch: 174, Loss: 0.000868\n",
            "Epoch: 175, Loss: 0.001582\n",
            "Epoch: 176, Loss: 0.001118\n",
            "Epoch: 177, Loss: 0.001469\n",
            "Epoch: 178, Loss: 0.001552\n",
            "Epoch: 179, Loss: 0.001525\n",
            "Epoch: 180, Loss: 0.000889\n",
            "Epoch: 181, Loss: 0.001390\n",
            "Epoch: 182, Loss: 0.001612\n",
            "Epoch: 183, Loss: 0.001521\n",
            "Epoch: 184, Loss: 0.001017\n",
            "Epoch: 185, Loss: 0.000888\n",
            "Epoch: 186, Loss: 0.001829\n",
            "Epoch: 187, Loss: 0.001205\n",
            "Epoch: 188, Loss: 0.001480\n",
            "Epoch: 189, Loss: 0.000789\n",
            "Epoch: 190, Loss: 0.001161\n",
            "Epoch: 191, Loss: 0.001462\n",
            "Epoch: 192, Loss: 0.001118\n",
            "Epoch: 193, Loss: 0.000780\n",
            "Epoch: 194, Loss: 0.001408\n",
            "Epoch: 195, Loss: 0.001644\n",
            "Epoch: 196, Loss: 0.000704\n",
            "Epoch: 197, Loss: 0.000713\n",
            "Epoch: 198, Loss: 0.000576\n",
            "Epoch: 199, Loss: 0.001678\n",
            "Epoch: 200, Loss: 0.000857\n",
            "Epoch: 201, Loss: 0.000835\n",
            "Epoch: 202, Loss: 0.001143\n",
            "Epoch: 203, Loss: 0.001283\n",
            "Epoch: 204, Loss: 0.001384\n",
            "Epoch: 205, Loss: 0.000661\n",
            "Epoch: 206, Loss: 0.000708\n",
            "Epoch: 207, Loss: 0.000554\n",
            "Epoch: 208, Loss: 0.001049\n",
            "Epoch: 209, Loss: 0.000972\n",
            "Epoch: 210, Loss: 0.000868\n",
            "Epoch: 211, Loss: 0.000592\n",
            "Epoch: 212, Loss: 0.001085\n",
            "Epoch: 213, Loss: 0.001101\n",
            "Epoch: 214, Loss: 0.000692\n",
            "Epoch: 215, Loss: 0.000678\n",
            "Epoch: 216, Loss: 0.001282\n",
            "Epoch: 217, Loss: 0.000721\n",
            "Epoch: 218, Loss: 0.001266\n",
            "Epoch: 219, Loss: 0.001143\n",
            "Epoch: 220, Loss: 0.001113\n",
            "Epoch: 221, Loss: 0.000579\n",
            "Epoch: 222, Loss: 0.000623\n",
            "Epoch: 223, Loss: 0.001014\n",
            "Epoch: 224, Loss: 0.000932\n",
            "Epoch: 225, Loss: 0.001228\n",
            "Epoch: 226, Loss: 0.000761\n",
            "Epoch: 227, Loss: 0.001010\n",
            "Epoch: 228, Loss: 0.001168\n",
            "Epoch: 229, Loss: 0.000481\n",
            "Epoch: 230, Loss: 0.001209\n",
            "Epoch: 231, Loss: 0.000958\n",
            "Epoch: 232, Loss: 0.000656\n",
            "Epoch: 233, Loss: 0.000826\n",
            "Epoch: 234, Loss: 0.000939\n",
            "Epoch: 235, Loss: 0.000631\n",
            "Epoch: 236, Loss: 0.001050\n",
            "Epoch: 237, Loss: 0.000539\n",
            "Epoch: 238, Loss: 0.000960\n",
            "Epoch: 239, Loss: 0.001032\n",
            "Epoch: 240, Loss: 0.001394\n",
            "Epoch: 241, Loss: 0.000663\n",
            "Epoch: 242, Loss: 0.000569\n",
            "Epoch: 243, Loss: 0.001788\n",
            "Epoch: 244, Loss: 0.000751\n",
            "Epoch: 245, Loss: 0.000646\n",
            "Epoch: 246, Loss: 0.000445\n",
            "Epoch: 247, Loss: 0.000613\n",
            "Epoch: 248, Loss: 0.000832\n",
            "Epoch: 249, Loss: 0.000547\n",
            "Epoch: 250, Loss: 0.001420\n",
            "Epoch: 251, Loss: 0.000843\n",
            "Epoch: 252, Loss: 0.000815\n",
            "Epoch: 253, Loss: 0.000524\n",
            "Epoch: 254, Loss: 0.000545\n",
            "Epoch: 255, Loss: 0.001170\n",
            "Epoch: 256, Loss: 0.000978\n",
            "Epoch: 257, Loss: 0.000579\n",
            "Epoch: 258, Loss: 0.000563\n",
            "Epoch: 259, Loss: 0.000927\n",
            "Epoch: 260, Loss: 0.000733\n",
            "Epoch: 261, Loss: 0.000542\n",
            "Epoch: 262, Loss: 0.000514\n",
            "Epoch: 263, Loss: 0.000611\n",
            "Epoch: 264, Loss: 0.000710\n",
            "Epoch: 265, Loss: 0.000668\n",
            "Epoch: 266, Loss: 0.000588\n",
            "Epoch: 267, Loss: 0.000744\n",
            "Epoch: 268, Loss: 0.000770\n",
            "Epoch: 269, Loss: 0.000940\n",
            "Epoch: 270, Loss: 0.000234\n",
            "Epoch: 271, Loss: 0.000486\n",
            "Epoch: 272, Loss: 0.000597\n",
            "Epoch: 273, Loss: 0.000321\n",
            "Epoch: 274, Loss: 0.000680\n",
            "Epoch: 275, Loss: 0.000484\n",
            "Epoch: 276, Loss: 0.000550\n",
            "Epoch: 277, Loss: 0.000728\n",
            "Epoch: 278, Loss: 0.000640\n",
            "Epoch: 279, Loss: 0.000868\n",
            "Epoch: 280, Loss: 0.000563\n",
            "Epoch: 281, Loss: 0.000449\n",
            "Epoch: 282, Loss: 0.000932\n",
            "Epoch: 283, Loss: 0.000595\n",
            "Epoch: 284, Loss: 0.000854\n",
            "Epoch: 285, Loss: 0.000374\n",
            "Epoch: 286, Loss: 0.000381\n",
            "Epoch: 287, Loss: 0.000388\n",
            "Epoch: 288, Loss: 0.000348\n",
            "Epoch: 289, Loss: 0.000598\n",
            "Epoch: 290, Loss: 0.000820\n",
            "Epoch: 291, Loss: 0.000440\n",
            "Epoch: 292, Loss: 0.000526\n",
            "Epoch: 293, Loss: 0.000801\n",
            "Epoch: 294, Loss: 0.000829\n",
            "Epoch: 295, Loss: 0.000551\n",
            "Epoch: 296, Loss: 0.000833\n",
            "Epoch: 297, Loss: 0.000330\n",
            "Epoch: 298, Loss: 0.000738\n",
            "Epoch: 299, Loss: 0.000324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = torch.utils.data.DataLoader(transformed_cifar10_val, batch_size=64,\n",
        "shuffle=False)\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for img, label in val_loader:\n",
        "    img = img.to(device)\n",
        "    label = label.to(device)\n",
        "    batch_size = img.shape[0]\n",
        "    outputs = model(img.view(batch_size, -1))\n",
        "    _, predicted = torch.max(outputs, dim=1)\n",
        "    total += label.shape[0]\n",
        "    correct += int((predicted == label).sum())\n",
        "print(\"Accuracy: %f\", correct / total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAN_f3Px7m2H",
        "outputId": "3bf54bd3-d905-4bcf-a843-2557178c220f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: %f 1.0\n"
          ]
        }
      ]
    }
  ]
}