{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1HK-o5GfwY36QnWIWBOjrOt2IG5fAhJht",
      "authorship_tag": "ABX9TyOtweNDO0mh0AlicDpe8AHN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oheckmann/Cloud/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGkUn6OYaZFd",
        "outputId": "d9e5e30c-dce1-429e-96a5-c4f8c3247085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[7.42e+03, 4.00e+00, 2.00e+00, 3.00e+00, 2.00e+00],\n",
              "       [8.96e+03, 4.00e+00, 4.00e+00, 4.00e+00, 3.00e+00],\n",
              "       [9.96e+03, 3.00e+00, 2.00e+00, 2.00e+00, 2.00e+00],\n",
              "       ...,\n",
              "       [3.62e+03, 2.00e+00, 1.00e+00, 1.00e+00, 0.00e+00],\n",
              "       [2.91e+03, 3.00e+00, 1.00e+00, 1.00e+00, 0.00e+00],\n",
              "       [3.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 0.00e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "house_path = \"drive/MyDrive/images/out.csv\"\n",
        "houseq_numpy2 = np.loadtxt(house_path, dtype=np.float, delimiter=\",\",\n",
        "skiprows=1, usecols=[1, 2,3,4,5])\n",
        "houseq_numpy2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "housing = pd.DataFrame(pd.read_csv(\"drive/MyDrive/images/Housing.csv\")) \n",
        "housing.head() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "oCEA0NrGgrZ5",
        "outputId": "e8733e4d-41c4-43c6-ffe9-dd406b489902"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3340cc81-be70-45c2-9e16-9fc4a0fabaf2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>area</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>stories</th>\n",
              "      <th>mainroad</th>\n",
              "      <th>guestroom</th>\n",
              "      <th>basement</th>\n",
              "      <th>hotwaterheating</th>\n",
              "      <th>airconditioning</th>\n",
              "      <th>parking</th>\n",
              "      <th>prefarea</th>\n",
              "      <th>furnishingstatus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>13300000</td>\n",
              "      <td>7420</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12250000</td>\n",
              "      <td>8960</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "      <td>no</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12250000</td>\n",
              "      <td>9960</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>semi-furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12215000</td>\n",
              "      <td>7500</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "      <td>yes</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11410000</td>\n",
              "      <td>7420</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "      <td>no</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3340cc81-be70-45c2-9e16-9fc4a0fabaf2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3340cc81-be70-45c2-9e16-9fc4a0fabaf2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3340cc81-be70-45c2-9e16-9fc4a0fabaf2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      price  area  bedrooms  ...  parking  prefarea furnishingstatus\n",
              "0  13300000  7420         4  ...        2       yes        furnished\n",
              "1  12250000  8960         4  ...        3        no        furnished\n",
              "2  12250000  9960         3  ...        2       yes   semi-furnished\n",
              "3  12215000  7500         4  ...        3       yes        furnished\n",
              "4  11410000  7420         4  ...        2        no        furnished\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking','price'] \n",
        "Newtrain = housing[num_vars] \n",
        "Newtrain.head() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "p_MelbhVgsWl",
        "outputId": "40a0c14f-2c5a-42f7-c7f2-96ade27de747"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a4dfcbf7-9c69-4ba2-8b16-8dd135215fbb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>area</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>stories</th>\n",
              "      <th>parking</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7420</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>13300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8960</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>12250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9960</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>12250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7500</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>12215000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7420</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>11410000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4dfcbf7-9c69-4ba2-8b16-8dd135215fbb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a4dfcbf7-9c69-4ba2-8b16-8dd135215fbb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a4dfcbf7-9c69-4ba2-8b16-8dd135215fbb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   area  bedrooms  bathrooms  stories  parking     price\n",
              "0  7420         4          2        3        2  13300000\n",
              "1  8960         4          4        4        3  12250000\n",
              "2  9960         3          2        2        2  12250000\n",
              "3  7500         4          2        2        3  12215000\n",
              "4  7420         4          1        2        2  11410000"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "house_path = \"drive/MyDrive/images/out.csv\"\n",
        "houseq_numpy = np.loadtxt(house_path, dtype=np.float, delimiter=\",\",\n",
        "skiprows=1, usecols=[6])\n",
        "houseq_numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHVBHfX7wcTh",
        "outputId": "95b24c2e-98f2-4ee3-90f1-8009d9b4484f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([13300000., 12250000., 12250000., 12215000., 11410000., 10850000.,\n",
              "       10150000., 10150000.,  9870000.,  9800000.,  9800000.,  9681000.,\n",
              "        9310000.,  9240000.,  9240000.,  9100000.,  9100000.,  8960000.,\n",
              "        8890000.,  8855000.,  8750000.,  8680000.,  8645000.,  8645000.,\n",
              "        8575000.,  8540000.,  8463000.,  8400000.,  8400000.,  8400000.,\n",
              "        8400000.,  8400000.,  8295000.,  8190000.,  8120000.,  8080940.,\n",
              "        8043000.,  7980000.,  7962500.,  7910000.,  7875000.,  7840000.,\n",
              "        7700000.,  7700000.,  7560000.,  7560000.,  7525000.,  7490000.,\n",
              "        7455000.,  7420000.,  7420000.,  7420000.,  7350000.,  7350000.,\n",
              "        7350000.,  7350000.,  7343000.,  7245000.,  7210000.,  7210000.,\n",
              "        7140000.,  7070000.,  7070000.,  7035000.,  7000000.,  6930000.,\n",
              "        6930000.,  6895000.,  6860000.,  6790000.,  6790000.,  6755000.,\n",
              "        6720000.,  6685000.,  6650000.,  6650000.,  6650000.,  6650000.,\n",
              "        6650000.,  6650000.,  6629000.,  6615000.,  6615000.,  6580000.,\n",
              "        6510000.,  6510000.,  6510000.,  6475000.,  6475000.,  6440000.,\n",
              "        6440000.,  6419000.,  6405000.,  6300000.,  6300000.,  6300000.,\n",
              "        6300000.,  6300000.,  6293000.,  6265000.,  6230000.,  6230000.,\n",
              "        6195000.,  6195000.,  6195000.,  6160000.,  6160000.,  6125000.,\n",
              "        6107500.,  6090000.,  6090000.,  6090000.,  6083000.,  6083000.,\n",
              "        6020000.,  6020000.,  6020000.,  5950000.,  5950000.,  5950000.,\n",
              "        5950000.,  5950000.,  5950000.,  5950000.,  5950000.,  5943000.,\n",
              "        5880000.,  5880000.,  5873000.,  5873000.,  5866000.,  5810000.,\n",
              "        5810000.,  5810000.,  5803000.,  5775000.,  5740000.,  5740000.,\n",
              "        5740000.,  5740000.,  5740000.,  5652500.,  5600000.,  5600000.,\n",
              "        5600000.,  5600000.,  5600000.,  5600000.,  5600000.,  5600000.,\n",
              "        5600000.,  5565000.,  5565000.,  5530000.,  5530000.,  5530000.,\n",
              "        5523000.,  5495000.,  5495000.,  5460000.,  5460000.,  5460000.,\n",
              "        5460000.,  5425000.,  5390000.,  5383000.,  5320000.,  5285000.,\n",
              "        5250000.,  5250000.,  5250000.,  5250000.,  5250000.,  5250000.,\n",
              "        5250000.,  5250000.,  5250000.,  5243000.,  5229000.,  5215000.,\n",
              "        5215000.,  5215000.,  5145000.,  5145000.,  5110000.,  5110000.,\n",
              "        5110000.,  5110000.,  5075000.,  5040000.,  5040000.,  5040000.,\n",
              "        5040000.,  5033000.,  5005000.,  4970000.,  4970000.,  4956000.,\n",
              "        4935000.,  4907000.,  4900000.,  4900000.,  4900000.,  4900000.,\n",
              "        4900000.,  4900000.,  4900000.,  4900000.,  4900000.,  4900000.,\n",
              "        4900000.,  4900000.,  4893000.,  4893000.,  4865000.,  4830000.,\n",
              "        4830000.,  4830000.,  4830000.,  4795000.,  4795000.,  4767000.,\n",
              "        4760000.,  4760000.,  4760000.,  4753000.,  4690000.,  4690000.,\n",
              "        4690000.,  4690000.,  4690000.,  4690000.,  4655000.,  4620000.,\n",
              "        4620000.,  4620000.,  4620000.,  4620000.,  4613000.,  4585000.,\n",
              "        4585000.,  4550000.,  4550000.,  4550000.,  4550000.,  4550000.,\n",
              "        4550000.,  4550000.,  4543000.,  4543000.,  4515000.,  4515000.,\n",
              "        4515000.,  4515000.,  4480000.,  4480000.,  4480000.,  4480000.,\n",
              "        4480000.,  4473000.,  4473000.,  4473000.,  4445000.,  4410000.,\n",
              "        4410000.,  4403000.,  4403000.,  4403000.,  4382000.,  4375000.,\n",
              "        4340000.,  4340000.,  4340000.,  4340000.,  4340000.,  4319000.,\n",
              "        4305000.,  4305000.,  4277000.,  4270000.,  4270000.,  4270000.,\n",
              "        4270000.,  4270000.,  4270000.,  4235000.,  4235000.,  4200000.,\n",
              "        4200000.,  4200000.,  4200000.,  4200000.,  4200000.,  4200000.,\n",
              "        4200000.,  4200000.,  4200000.,  4200000.,  4200000.,  4200000.,\n",
              "        4200000.,  4200000.,  4200000.,  4200000.,  4193000.,  4193000.,\n",
              "        4165000.,  4165000.,  4165000.,  4130000.,  4130000.,  4123000.,\n",
              "        4098500.,  4095000.,  4095000.,  4095000.,  4060000.,  4060000.,\n",
              "        4060000.,  4060000.,  4060000.,  4025000.,  4025000.,  4025000.,\n",
              "        4007500.,  4007500.,  3990000.,  3990000.,  3990000.,  3990000.,\n",
              "        3990000.,  3920000.,  3920000.,  3920000.,  3920000.,  3920000.,\n",
              "        3920000.,  3920000.,  3885000.,  3885000.,  3850000.,  3850000.,\n",
              "        3850000.,  3850000.,  3850000.,  3850000.,  3850000.,  3836000.,\n",
              "        3815000.,  3780000.,  3780000.,  3780000.,  3780000.,  3780000.,\n",
              "        3780000.,  3773000.,  3773000.,  3773000.,  3745000.,  3710000.,\n",
              "        3710000.,  3710000.,  3710000.,  3710000.,  3703000.,  3703000.,\n",
              "        3675000.,  3675000.,  3675000.,  3675000.,  3640000.,  3640000.,\n",
              "        3640000.,  3640000.,  3640000.,  3640000.,  3640000.,  3640000.,\n",
              "        3640000.,  3633000.,  3605000.,  3605000.,  3570000.,  3570000.,\n",
              "        3570000.,  3570000.,  3535000.,  3500000.,  3500000.,  3500000.,\n",
              "        3500000.,  3500000.,  3500000.,  3500000.,  3500000.,  3500000.,\n",
              "        3500000.,  3500000.,  3500000.,  3500000.,  3500000.,  3500000.,\n",
              "        3500000.,  3500000.,  3493000.,  3465000.,  3465000.,  3465000.,\n",
              "        3430000.,  3430000.,  3430000.,  3430000.,  3430000.,  3430000.,\n",
              "        3423000.,  3395000.,  3395000.,  3395000.,  3360000.,  3360000.,\n",
              "        3360000.,  3360000.,  3360000.,  3360000.,  3360000.,  3360000.,\n",
              "        3353000.,  3332000.,  3325000.,  3325000.,  3290000.,  3290000.,\n",
              "        3290000.,  3290000.,  3290000.,  3290000.,  3290000.,  3290000.,\n",
              "        3255000.,  3255000.,  3234000.,  3220000.,  3220000.,  3220000.,\n",
              "        3220000.,  3150000.,  3150000.,  3150000.,  3150000.,  3150000.,\n",
              "        3150000.,  3150000.,  3150000.,  3150000.,  3143000.,  3129000.,\n",
              "        3118850.,  3115000.,  3115000.,  3115000.,  3087000.,  3080000.,\n",
              "        3080000.,  3080000.,  3080000.,  3045000.,  3010000.,  3010000.,\n",
              "        3010000.,  3010000.,  3010000.,  3010000.,  3010000.,  3003000.,\n",
              "        2975000.,  2961000.,  2940000.,  2940000.,  2940000.,  2940000.,\n",
              "        2940000.,  2940000.,  2940000.,  2940000.,  2870000.,  2870000.,\n",
              "        2870000.,  2870000.,  2852500.,  2835000.,  2835000.,  2835000.,\n",
              "        2800000.,  2800000.,  2730000.,  2730000.,  2695000.,  2660000.,\n",
              "        2660000.,  2660000.,  2660000.,  2660000.,  2660000.,  2660000.,\n",
              "        2653000.,  2653000.,  2604000.,  2590000.,  2590000.,  2590000.,\n",
              "        2520000.,  2520000.,  2520000.,  2485000.,  2485000.,  2450000.,\n",
              "        2450000.,  2450000.,  2450000.,  2450000.,  2450000.,  2408000.,\n",
              "        2380000.,  2380000.,  2380000.,  2345000.,  2310000.,  2275000.,\n",
              "        2275000.,  2275000.,  2240000.,  2233000.,  2135000.,  2100000.,\n",
              "        2100000.,  2100000.,  1960000.,  1890000.,  1890000.,  1855000.,\n",
              "        1820000.,  1767150.,  1750000.,  1750000.,  1750000.])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tc = torch.from_numpy(houseq_numpy)\n",
        "tc = torch.tensor(tc.float()).unsqueeze(1)\n",
        "tc.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5Pmn9I92I0i",
        "outputId": "2287cfd1-897c-4d3d-aae3-edf0448dc157"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([545, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tu = torch.from_numpy(houseq_numpy2)\n",
        "tu = torch.tensor(tu.float())\n",
        "tu.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gn14U4Z75SgL",
        "outputId": "e5d1d0db-7965-42bb-eb7e-de1ec7790165"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([545, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
        "t_price = tc\n",
        "t_data = tu\n",
        "\n",
        "\n",
        "t_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jOIwVjFwg-6",
        "outputId": "511fdce0-237a-4a43-b6b6-10d61dbe34d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([545, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(x):\n",
        "    x_normed = x / x.max(0, keepdim=True)[0]\n",
        "    return x_normed"
      ],
      "metadata": {
        "id": "-cYLgZyMWKYn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_norm = normalize(t_data)\n",
        "tc = normalize(tc)"
      ],
      "metadata": {
        "id": "I234lJphWMlO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = t_norm.shape[0]\n",
        "n_val = int(0.2 * n_samples)\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "train_indices = shuffled_indices[n_val:]\n",
        "val_indices = shuffled_indices[:n_val]\n",
        "train_indices, val_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYcohGhukcvH",
        "outputId": "caffb330-30dc-40ff-e76b-31d63605693e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([267, 541,  63,  37, 209, 517, 152, 327,  29, 503, 232, 118, 200,\n",
              "          74, 297, 511, 323, 304, 190, 408, 411, 492, 134, 234, 409, 449,\n",
              "         391, 179, 103, 381, 139, 299, 402, 403,  62, 342, 512, 373, 148,\n",
              "         407, 269, 504, 106,  32, 115, 324, 248,  27, 462, 197, 149, 287,\n",
              "         107, 245, 507, 508,  89, 207, 320, 356,  70,  76, 237,  41,  18,\n",
              "         182, 307,  61, 499, 206, 293, 522, 283, 181, 431, 136,   4, 146,\n",
              "         424, 183,  60, 472, 316,  91,  80,  86, 249, 535, 384, 343, 244,\n",
              "         254,   8, 276, 529, 120, 203, 165, 364, 154, 260,   9, 162, 521,\n",
              "          34, 147, 486, 360, 161, 460, 488, 526, 295, 150, 271, 131,  51,\n",
              "         426, 377, 474, 452, 432, 338, 328, 444, 429, 281, 121, 459, 225,\n",
              "          79, 191, 401,  75,  72, 398, 226, 104, 168, 332,  23,  43,  85,\n",
              "         144, 300, 530, 337, 365, 453, 239, 467, 186, 296, 241, 240,   1,\n",
              "          13,  50, 236,  25, 253, 476, 441, 415,   6, 102, 187, 220, 301,\n",
              "         390, 397, 388, 315, 532, 379, 310, 202, 135, 538, 523, 428, 481,\n",
              "         137,  14, 126, 527, 199,  24,  56, 212, 510, 298, 465, 217, 502,\n",
              "         473, 495, 478,  69, 536, 386, 125, 266, 173, 466, 425, 539, 440,\n",
              "         483, 387, 108, 171,  31, 494, 506,   5,  16,   7, 513,  98, 524,\n",
              "         308, 221, 497, 357, 496, 416, 184, 533, 351, 455, 361, 109, 288,\n",
              "         292, 392,  67, 372, 258,  78, 247, 141,  44, 282,  57, 111, 166,\n",
              "         363, 389, 178, 156, 439, 450, 227, 122, 124, 359,  33, 224, 167,\n",
              "         509,  59, 289, 251, 457, 326, 263, 305, 159,  19, 322, 132,  36,\n",
              "          10,  46, 262, 456, 210, 188, 469, 378, 458, 194,  47, 259, 375,\n",
              "         383, 412, 451,  77, 185, 268, 352, 537,  93, 216, 484, 145, 176,\n",
              "         362, 345,  42, 376, 306,   3, 100, 314, 487,  28, 454, 490, 433,\n",
              "         370, 233, 235,  12, 279, 329, 128, 519,  20,  71, 110, 422, 430,\n",
              "          45, 421, 518, 468, 158, 353, 367, 336, 192,  65, 205, 350, 347,\n",
              "         273, 255, 193, 382, 257, 393, 471,  87, 417, 284,  52, 238,  17,\n",
              "         480,  35, 319,  54, 294, 264, 211, 317, 399, 371, 540,  64, 418,\n",
              "          49, 479, 446,  68, 204, 129, 231, 400, 261, 442, 170, 406, 312,\n",
              "         516, 348,  83, 198,  97,  48, 405, 133, 413, 344,   0, 214, 274,\n",
              "         525, 230, 114, 278, 445, 275, 101, 302,  40, 470, 272, 447, 443,\n",
              "         151,  82, 243, 160, 368, 334, 195,  73, 223,  22, 528, 112, 335,\n",
              "         163, 218, 477,  96, 410,  53, 485, 420, 143, 355, 385, 246, 172,\n",
              "          21, 252, 482, 463,  15,  26, 434]),\n",
              " tensor([127, 138, 325, 280, 543, 542,  88, 130, 196, 340, 175,  94, 155,\n",
              "         534, 229, 419, 286, 330, 520, 169,  92, 219, 346,  95, 427, 123,\n",
              "         423, 491, 318, 113, 493,  39, 395, 461,  81, 515, 514, 531, 438,\n",
              "         277, 501, 380, 331, 366, 291, 174, 354, 119,  66, 201, 189, 333,\n",
              "         321, 164,  30,  11,  99, 290, 142, 177, 435, 265, 228, 358,  90,\n",
              "         105, 396,  38, 311, 394, 374, 341, 544, 500, 180, 404, 208, 242,\n",
              "         498, 505, 349, 285, 303, 250, 222, 309, 436, 270, 215, 369, 116,\n",
              "         489,   2, 256, 313, 448, 153,  84, 117, 464, 414, 157, 213,  58,\n",
              "         339, 140,  55, 437, 475]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_t_u = t_norm[train_indices]\n",
        "train_t_c = tc[train_indices]\n",
        "val_t_u = t_norm[val_indices]\n",
        "val_t_c = tc[val_indices]\n",
        "train_t_un = 0.1 * train_t_u\n",
        "val_t_un = 0.1 * val_t_u"
      ],
      "metadata": {
        "id": "Hoy3QalZo5su"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "from collections import OrderedDict\n",
        "seq_model = nn.Sequential(OrderedDict([\n",
        "('hidden_linear', nn.Linear(5, 8)),\n",
        "('hidden_activation', nn.Tanh()),\n",
        "('output_linear', nn.Linear(8, 1))\n",
        "]))\n",
        "seq_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TakeMVW28nJz",
        "outputId": "8f06d3ad-33cb-4aaf-dc6e-77fdd6273198"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (hidden_linear): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (hidden_activation): Tanh()\n",
              "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = seq_model(train_t_un)\n",
        "loss = nn.MSELoss()\n",
        "loss(d, train_t_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoZxSltKDISE",
        "outputId": "31f87be0-40eb-4c32-92b8-b6ed4bf1ae0d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0314, grad_fn=<MseLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in seq_model.named_parameters():\n",
        "  print(name, param.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG4QEY_28wTX",
        "outputId": "73735eac-89d1-4d32-8a58-64e1c9bf091d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden_linear.weight torch.Size([8, 5])\n",
            "hidden_linear.bias torch.Size([8])\n",
            "output_linear.weight torch.Size([1, 8])\n",
            "output_linear.bias torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[param.shape for param in seq_model.parameters()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHb7jUFu802g",
        "outputId": "b062e590-acb1-42f7-c2c8-d3a69048a1f9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([8, 5]), torch.Size([8]), torch.Size([1, 8]), torch.Size([1])]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train, t_c_val):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    t_p_train = model(t_u_train)\n",
        "    loss_train = loss_fn(t_p_train, t_c_train)\n",
        "    t_p_val = model(t_u_val)\n",
        "    loss_val = loss_fn(t_p_val, t_c_val)\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    if epoch == 1 or epoch % 10 == 0:\n",
        "      print(f\"Epoch {epoch}, Training loss {loss_train.item():.4f},\" f\" Validation loss {loss_val.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "H7EheJDBi_2f"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(seq_model.parameters(), lr=1e-5)\n",
        "\n",
        "training_loop(\n",
        "n_epochs = 200,\n",
        "optimizer = optimizer,\n",
        "model = seq_model,\n",
        "loss_fn = nn.MSELoss(),\n",
        "t_u_train = train_t_un,\n",
        "t_u_val = val_t_un,\n",
        "t_c_train = train_t_c,\n",
        "t_c_val = val_t_c)\n",
        "seq_model.parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0a_DTBTjs00",
        "outputId": "ecbf10bf-0f65-4fb8-a7c9-edd8cab5236b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 0.0314, Validation loss 0.0231\n",
            "Epoch 10, Training loss 0.0314, Validation loss 0.0231\n",
            "Epoch 20, Training loss 0.0312, Validation loss 0.0230\n",
            "Epoch 30, Training loss 0.0311, Validation loss 0.0229\n",
            "Epoch 40, Training loss 0.0308, Validation loss 0.0227\n",
            "Epoch 50, Training loss 0.0305, Validation loss 0.0224\n",
            "Epoch 60, Training loss 0.0302, Validation loss 0.0221\n",
            "Epoch 70, Training loss 0.0297, Validation loss 0.0218\n",
            "Epoch 80, Training loss 0.0293, Validation loss 0.0214\n",
            "Epoch 90, Training loss 0.0287, Validation loss 0.0210\n",
            "Epoch 100, Training loss 0.0282, Validation loss 0.0206\n",
            "Epoch 110, Training loss 0.0276, Validation loss 0.0202\n",
            "Epoch 120, Training loss 0.0270, Validation loss 0.0197\n",
            "Epoch 130, Training loss 0.0264, Validation loss 0.0193\n",
            "Epoch 140, Training loss 0.0259, Validation loss 0.0188\n",
            "Epoch 150, Training loss 0.0253, Validation loss 0.0184\n",
            "Epoch 160, Training loss 0.0247, Validation loss 0.0181\n",
            "Epoch 170, Training loss 0.0242, Validation loss 0.0177\n",
            "Epoch 180, Training loss 0.0237, Validation loss 0.0174\n",
            "Epoch 190, Training loss 0.0232, Validation loss 0.0171\n",
            "Epoch 200, Training loss 0.0228, Validation loss 0.0169\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of Sequential(\n",
              "  (hidden_linear): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (hidden_activation): Tanh()\n",
              "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_model(train_t_un)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3EylhwZ6TGM",
        "outputId": "4ee23d01-dd7d-4fba-aa57-f24ab2886ba0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3269],\n",
              "        [0.3355],\n",
              "        [0.3180],\n",
              "        [0.3135],\n",
              "        [0.3326],\n",
              "        [0.3334],\n",
              "        [0.3258],\n",
              "        [0.3259],\n",
              "        [0.3255],\n",
              "        [0.3344],\n",
              "        [0.3319],\n",
              "        [0.3328],\n",
              "        [0.3313],\n",
              "        [0.3275],\n",
              "        [0.3333],\n",
              "        [0.3349],\n",
              "        [0.3211],\n",
              "        [0.3191],\n",
              "        [0.3185],\n",
              "        [0.3369],\n",
              "        [0.3302],\n",
              "        [0.3284],\n",
              "        [0.3241],\n",
              "        [0.3249],\n",
              "        [0.3291],\n",
              "        [0.3333],\n",
              "        [0.3323],\n",
              "        [0.3254],\n",
              "        [0.3288],\n",
              "        [0.3369],\n",
              "        [0.3245],\n",
              "        [0.3200],\n",
              "        [0.3356],\n",
              "        [0.3284],\n",
              "        [0.3250],\n",
              "        [0.3296],\n",
              "        [0.3348],\n",
              "        [0.3348],\n",
              "        [0.3274],\n",
              "        [0.3302],\n",
              "        [0.3317],\n",
              "        [0.3374],\n",
              "        [0.3324],\n",
              "        [0.3259],\n",
              "        [0.3234],\n",
              "        [0.3313],\n",
              "        [0.3382],\n",
              "        [0.3270],\n",
              "        [0.3329],\n",
              "        [0.3297],\n",
              "        [0.3316],\n",
              "        [0.3307],\n",
              "        [0.3274],\n",
              "        [0.3225],\n",
              "        [0.3371],\n",
              "        [0.3366],\n",
              "        [0.3183],\n",
              "        [0.3323],\n",
              "        [0.3232],\n",
              "        [0.3278],\n",
              "        [0.3331],\n",
              "        [0.3287],\n",
              "        [0.3310],\n",
              "        [0.3261],\n",
              "        [0.3244],\n",
              "        [0.3321],\n",
              "        [0.3233],\n",
              "        [0.3294],\n",
              "        [0.3347],\n",
              "        [0.3356],\n",
              "        [0.3324],\n",
              "        [0.3327],\n",
              "        [0.3290],\n",
              "        [0.3254],\n",
              "        [0.3298],\n",
              "        [0.3214],\n",
              "        [0.3186],\n",
              "        [0.3283],\n",
              "        [0.3323],\n",
              "        [0.3276],\n",
              "        [0.3276],\n",
              "        [0.3171],\n",
              "        [0.3252],\n",
              "        [0.3267],\n",
              "        [0.3262],\n",
              "        [0.3272],\n",
              "        [0.3299],\n",
              "        [0.3332],\n",
              "        [0.3365],\n",
              "        [0.3368],\n",
              "        [0.3308],\n",
              "        [0.3206],\n",
              "        [0.3182],\n",
              "        [0.3324],\n",
              "        [0.3317],\n",
              "        [0.3244],\n",
              "        [0.3283],\n",
              "        [0.3341],\n",
              "        [0.3323],\n",
              "        [0.3251],\n",
              "        [0.3281],\n",
              "        [0.3223],\n",
              "        [0.3288],\n",
              "        [0.3371],\n",
              "        [0.3207],\n",
              "        [0.3279],\n",
              "        [0.3355],\n",
              "        [0.3368],\n",
              "        [0.3276],\n",
              "        [0.3300],\n",
              "        [0.3257],\n",
              "        [0.3374],\n",
              "        [0.3328],\n",
              "        [0.3309],\n",
              "        [0.3281],\n",
              "        [0.3226],\n",
              "        [0.3206],\n",
              "        [0.3353],\n",
              "        [0.3338],\n",
              "        [0.3248],\n",
              "        [0.3200],\n",
              "        [0.3330],\n",
              "        [0.3343],\n",
              "        [0.3327],\n",
              "        [0.3323],\n",
              "        [0.3287],\n",
              "        [0.3283],\n",
              "        [0.3295],\n",
              "        [0.3372],\n",
              "        [0.3228],\n",
              "        [0.3290],\n",
              "        [0.3272],\n",
              "        [0.3155],\n",
              "        [0.3304],\n",
              "        [0.3256],\n",
              "        [0.3281],\n",
              "        [0.3255],\n",
              "        [0.3266],\n",
              "        [0.3290],\n",
              "        [0.3282],\n",
              "        [0.3286],\n",
              "        [0.3155],\n",
              "        [0.3275],\n",
              "        [0.3246],\n",
              "        [0.3289],\n",
              "        [0.3358],\n",
              "        [0.3346],\n",
              "        [0.3359],\n",
              "        [0.3334],\n",
              "        [0.3275],\n",
              "        [0.3239],\n",
              "        [0.3292],\n",
              "        [0.3285],\n",
              "        [0.3263],\n",
              "        [0.3276],\n",
              "        [0.3123],\n",
              "        [0.3227],\n",
              "        [0.3212],\n",
              "        [0.3349],\n",
              "        [0.3206],\n",
              "        [0.3345],\n",
              "        [0.3263],\n",
              "        [0.3314],\n",
              "        [0.3270],\n",
              "        [0.3152],\n",
              "        [0.3225],\n",
              "        [0.3330],\n",
              "        [0.3127],\n",
              "        [0.3320],\n",
              "        [0.3343],\n",
              "        [0.3314],\n",
              "        [0.3319],\n",
              "        [0.3358],\n",
              "        [0.3375],\n",
              "        [0.3265],\n",
              "        [0.3331],\n",
              "        [0.3326],\n",
              "        [0.3263],\n",
              "        [0.3371],\n",
              "        [0.3314],\n",
              "        [0.3327],\n",
              "        [0.3377],\n",
              "        [0.3246],\n",
              "        [0.3305],\n",
              "        [0.3240],\n",
              "        [0.3383],\n",
              "        [0.3274],\n",
              "        [0.3216],\n",
              "        [0.3201],\n",
              "        [0.3228],\n",
              "        [0.3351],\n",
              "        [0.3249],\n",
              "        [0.3370],\n",
              "        [0.3215],\n",
              "        [0.3331],\n",
              "        [0.3341],\n",
              "        [0.3275],\n",
              "        [0.3278],\n",
              "        [0.3169],\n",
              "        [0.3271],\n",
              "        [0.3262],\n",
              "        [0.3183],\n",
              "        [0.3255],\n",
              "        [0.3325],\n",
              "        [0.3323],\n",
              "        [0.3291],\n",
              "        [0.3334],\n",
              "        [0.3295],\n",
              "        [0.3299],\n",
              "        [0.3315],\n",
              "        [0.3229],\n",
              "        [0.3260],\n",
              "        [0.3160],\n",
              "        [0.3350],\n",
              "        [0.3376],\n",
              "        [0.3266],\n",
              "        [0.3247],\n",
              "        [0.3213],\n",
              "        [0.3314],\n",
              "        [0.3286],\n",
              "        [0.3374],\n",
              "        [0.3275],\n",
              "        [0.3310],\n",
              "        [0.3369],\n",
              "        [0.3231],\n",
              "        [0.3369],\n",
              "        [0.3265],\n",
              "        [0.3337],\n",
              "        [0.3327],\n",
              "        [0.3371],\n",
              "        [0.3334],\n",
              "        [0.3330],\n",
              "        [0.3247],\n",
              "        [0.3310],\n",
              "        [0.3314],\n",
              "        [0.3317],\n",
              "        [0.3251],\n",
              "        [0.3322],\n",
              "        [0.3275],\n",
              "        [0.3250],\n",
              "        [0.3084],\n",
              "        [0.3247],\n",
              "        [0.3197],\n",
              "        [0.3329],\n",
              "        [0.3177],\n",
              "        [0.3177],\n",
              "        [0.3236],\n",
              "        [0.3371],\n",
              "        [0.3288],\n",
              "        [0.3240],\n",
              "        [0.3324],\n",
              "        [0.3369],\n",
              "        [0.3320],\n",
              "        [0.3314],\n",
              "        [0.3277],\n",
              "        [0.3218],\n",
              "        [0.3305],\n",
              "        [0.3290],\n",
              "        [0.3244],\n",
              "        [0.3296],\n",
              "        [0.3317],\n",
              "        [0.3221],\n",
              "        [0.3273],\n",
              "        [0.3279],\n",
              "        [0.3351],\n",
              "        [0.3319],\n",
              "        [0.3317],\n",
              "        [0.3279],\n",
              "        [0.3364],\n",
              "        [0.3273],\n",
              "        [0.3321],\n",
              "        [0.3282],\n",
              "        [0.3239],\n",
              "        [0.3172],\n",
              "        [0.3221],\n",
              "        [0.3370],\n",
              "        [0.3355],\n",
              "        [0.3230],\n",
              "        [0.3330],\n",
              "        [0.3365],\n",
              "        [0.3301],\n",
              "        [0.3318],\n",
              "        [0.3330],\n",
              "        [0.3121],\n",
              "        [0.3311],\n",
              "        [0.3304],\n",
              "        [0.3220],\n",
              "        [0.3326],\n",
              "        [0.3350],\n",
              "        [0.3287],\n",
              "        [0.3323],\n",
              "        [0.3286],\n",
              "        [0.3273],\n",
              "        [0.3332],\n",
              "        [0.3213],\n",
              "        [0.3247],\n",
              "        [0.3375],\n",
              "        [0.3256],\n",
              "        [0.3231],\n",
              "        [0.3368],\n",
              "        [0.3330],\n",
              "        [0.3177],\n",
              "        [0.3247],\n",
              "        [0.3270],\n",
              "        [0.3159],\n",
              "        [0.3340],\n",
              "        [0.3300],\n",
              "        [0.3283],\n",
              "        [0.3172],\n",
              "        [0.3313],\n",
              "        [0.3273],\n",
              "        [0.3254],\n",
              "        [0.3284],\n",
              "        [0.3343],\n",
              "        [0.3264],\n",
              "        [0.3248],\n",
              "        [0.3353],\n",
              "        [0.3317],\n",
              "        [0.3238],\n",
              "        [0.3336],\n",
              "        [0.3232],\n",
              "        [0.3238],\n",
              "        [0.3244],\n",
              "        [0.3370],\n",
              "        [0.3379],\n",
              "        [0.3290],\n",
              "        [0.3364],\n",
              "        [0.3372],\n",
              "        [0.3376],\n",
              "        [0.3234],\n",
              "        [0.3297],\n",
              "        [0.3371],\n",
              "        [0.3234],\n",
              "        [0.3326],\n",
              "        [0.3256],\n",
              "        [0.3246],\n",
              "        [0.3304],\n",
              "        [0.3321],\n",
              "        [0.3213],\n",
              "        [0.3314],\n",
              "        [0.3363],\n",
              "        [0.3322],\n",
              "        [0.3315],\n",
              "        [0.3321],\n",
              "        [0.3318],\n",
              "        [0.3262],\n",
              "        [0.3371],\n",
              "        [0.3302],\n",
              "        [0.3197],\n",
              "        [0.3302],\n",
              "        [0.3163],\n",
              "        [0.3279],\n",
              "        [0.3173],\n",
              "        [0.3189],\n",
              "        [0.3276],\n",
              "        [0.3306],\n",
              "        [0.3336],\n",
              "        [0.3201],\n",
              "        [0.3242],\n",
              "        [0.3305],\n",
              "        [0.3320],\n",
              "        [0.3293],\n",
              "        [0.3254],\n",
              "        [0.3213],\n",
              "        [0.3335],\n",
              "        [0.3294],\n",
              "        [0.3341],\n",
              "        [0.3289],\n",
              "        [0.3339],\n",
              "        [0.3157],\n",
              "        [0.3342],\n",
              "        [0.3331],\n",
              "        [0.3320],\n",
              "        [0.3336],\n",
              "        [0.3348],\n",
              "        [0.3318],\n",
              "        [0.3347],\n",
              "        [0.3332],\n",
              "        [0.3388],\n",
              "        [0.3263],\n",
              "        [0.3304],\n",
              "        [0.3286],\n",
              "        [0.3287],\n",
              "        [0.3350],\n",
              "        [0.3284],\n",
              "        [0.3344],\n",
              "        [0.3370],\n",
              "        [0.3173],\n",
              "        [0.3366],\n",
              "        [0.3276],\n",
              "        [0.3371],\n",
              "        [0.3332],\n",
              "        [0.3267],\n",
              "        [0.3238],\n",
              "        [0.3397],\n",
              "        [0.3368],\n",
              "        [0.3238],\n",
              "        [0.3261],\n",
              "        [0.3300],\n",
              "        [0.3337],\n",
              "        [0.3261],\n",
              "        [0.3372],\n",
              "        [0.3300],\n",
              "        [0.3207],\n",
              "        [0.3273],\n",
              "        [0.3326],\n",
              "        [0.3223],\n",
              "        [0.3358],\n",
              "        [0.3332],\n",
              "        [0.3236],\n",
              "        [0.3284],\n",
              "        [0.3260],\n",
              "        [0.3275],\n",
              "        [0.3393],\n",
              "        [0.3255],\n",
              "        [0.3287],\n",
              "        [0.3325],\n",
              "        [0.3363],\n",
              "        [0.3362],\n",
              "        [0.3269],\n",
              "        [0.3345],\n",
              "        [0.3186],\n",
              "        [0.3371],\n",
              "        [0.3341],\n",
              "        [0.3249],\n",
              "        [0.3232],\n",
              "        [0.3371],\n",
              "        [0.3347],\n",
              "        [0.3204],\n",
              "        [0.3254],\n",
              "        [0.3305],\n",
              "        [0.3322],\n",
              "        [0.3375],\n",
              "        [0.3196],\n",
              "        [0.3263],\n",
              "        [0.3293]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "seq_model2 = nn.Sequential(OrderedDict([\n",
        "('hidden_linear', nn.Linear(5, 8)),\n",
        "('hidden_activation', nn.Tanh()),\n",
        "('hidden_linear2', nn.Linear(8, 8)),\n",
        "('hidden_activation2', nn.Tanh()),\n",
        "('output_linear', nn.Linear(8, 1))\n",
        "]))\n",
        "seq_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMxB-752qQBO",
        "outputId": "1ee8f253-42a4-4d09-867f-05ea20f3055d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (hidden_linear): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (hidden_activation): Tanh()\n",
              "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(seq_model2.parameters(), lr=1e-5)\n",
        "\n",
        "training_loop(\n",
        "n_epochs = 200,\n",
        "optimizer = optimizer,\n",
        "model = seq_model2,\n",
        "loss_fn = nn.MSELoss(),\n",
        "t_u_train = train_t_un,\n",
        "t_u_val = val_t_un,\n",
        "t_c_train = train_t_c,\n",
        "t_c_val = val_t_c)\n",
        "seq_model.parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK5zxdum8buj",
        "outputId": "912a78fc-3488-4a0b-f33e-470c96f92c66"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 0.2998, Validation loss 0.2769\n",
            "Epoch 10, Training loss 0.2989, Validation loss 0.2761\n",
            "Epoch 20, Training loss 0.2960, Validation loss 0.2733\n",
            "Epoch 30, Training loss 0.2912, Validation loss 0.2685\n",
            "Epoch 40, Training loss 0.2844, Validation loss 0.2620\n",
            "Epoch 50, Training loss 0.2758, Validation loss 0.2537\n",
            "Epoch 60, Training loss 0.2656, Validation loss 0.2438\n",
            "Epoch 70, Training loss 0.2538, Validation loss 0.2325\n",
            "Epoch 80, Training loss 0.2406, Validation loss 0.2198\n",
            "Epoch 90, Training loss 0.2262, Validation loss 0.2060\n",
            "Epoch 100, Training loss 0.2109, Validation loss 0.1912\n",
            "Epoch 110, Training loss 0.1947, Validation loss 0.1757\n",
            "Epoch 120, Training loss 0.1780, Validation loss 0.1597\n",
            "Epoch 130, Training loss 0.1610, Validation loss 0.1435\n",
            "Epoch 140, Training loss 0.1440, Validation loss 0.1272\n",
            "Epoch 150, Training loss 0.1271, Validation loss 0.1112\n",
            "Epoch 160, Training loss 0.1107, Validation loss 0.0957\n",
            "Epoch 170, Training loss 0.0949, Validation loss 0.0810\n",
            "Epoch 180, Training loss 0.0802, Validation loss 0.0672\n",
            "Epoch 190, Training loss 0.0666, Validation loss 0.0546\n",
            "Epoch 200, Training loss 0.0543, Validation loss 0.0435\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of Sequential(\n",
              "  (hidden_linear): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (hidden_activation): Tanh()\n",
              "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "seq_model3 = nn.Sequential(OrderedDict([\n",
        "('hidden_linear', nn.Linear(5, 8)),\n",
        "('hidden_activation', nn.Tanh()),\n",
        "('hidden_linear2', nn.Linear(8, 16)),\n",
        "('hidden_activation2', nn.Tanh()),\n",
        "('hidden_linear3', nn.Linear(16, 8)),\n",
        "('hidden_activation3', nn.Tanh()),\n",
        "('output_linear', nn.Linear(8, 1))\n",
        "]))\n",
        "seq_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mxJ-iHj82cf",
        "outputId": "6e7d174c-6307-4988-d07f-0d9f93491117"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (hidden_linear): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (hidden_activation): Tanh()\n",
              "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(seq_model3.parameters(), lr=1e-5)\n",
        "\n",
        "training_loop(\n",
        "n_epochs = 200,\n",
        "optimizer = optimizer,\n",
        "model = seq_model3,\n",
        "loss_fn = nn.MSELoss(),\n",
        "t_u_train = train_t_un,\n",
        "t_u_val = val_t_un,\n",
        "t_c_train = train_t_c,\n",
        "t_c_val = val_t_c)\n",
        "seq_model.parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6w7eBMb9AlY",
        "outputId": "9048f838-ca74-4fc0-be0a-65964c3995dd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 0.0395, Validation loss 0.0302\n",
            "Epoch 10, Training loss 0.0394, Validation loss 0.0301\n",
            "Epoch 20, Training loss 0.0392, Validation loss 0.0300\n",
            "Epoch 30, Training loss 0.0389, Validation loss 0.0297\n",
            "Epoch 40, Training loss 0.0385, Validation loss 0.0293\n",
            "Epoch 50, Training loss 0.0379, Validation loss 0.0288\n",
            "Epoch 60, Training loss 0.0372, Validation loss 0.0282\n",
            "Epoch 70, Training loss 0.0364, Validation loss 0.0275\n",
            "Epoch 80, Training loss 0.0355, Validation loss 0.0268\n",
            "Epoch 90, Training loss 0.0346, Validation loss 0.0259\n",
            "Epoch 100, Training loss 0.0335, Validation loss 0.0251\n",
            "Epoch 110, Training loss 0.0325, Validation loss 0.0242\n",
            "Epoch 120, Training loss 0.0314, Validation loss 0.0232\n",
            "Epoch 130, Training loss 0.0302, Validation loss 0.0223\n",
            "Epoch 140, Training loss 0.0291, Validation loss 0.0214\n",
            "Epoch 150, Training loss 0.0280, Validation loss 0.0205\n",
            "Epoch 160, Training loss 0.0269, Validation loss 0.0197\n",
            "Epoch 170, Training loss 0.0259, Validation loss 0.0189\n",
            "Epoch 180, Training loss 0.0249, Validation loss 0.0181\n",
            "Epoch 190, Training loss 0.0240, Validation loss 0.0175\n",
            "Epoch 200, Training loss 0.0232, Validation loss 0.0170\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of Sequential(\n",
              "  (hidden_linear): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (hidden_activation): Tanh()\n",
              "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "seq_model4 = nn.Sequential(OrderedDict([\n",
        "('hidden_linear', nn.Linear(5, 8)),\n",
        "('hidden_activation', nn.Tanh()),\n",
        "('hidden_linear2', nn.Linear(8, 16)),\n",
        "('hidden_activation2', nn.Tanh()),\n",
        "('hidden_linear3', nn.Linear(16, 64)),\n",
        "('hidden_activation3', nn.Tanh()),\n",
        "('hidden_linear4', nn.Linear(64, 16)),\n",
        "('hidden_activation4', nn.Tanh()),\n",
        "('output_linear', nn.Linear(16, 1))\n",
        "]))\n",
        "\n",
        "optimizer = optim.SGD(seq_model4.parameters(), lr=1e-5)\n",
        "\n",
        "training_loop(\n",
        "n_epochs = 200,\n",
        "optimizer = optimizer,\n",
        "model = seq_model4,\n",
        "loss_fn = nn.MSELoss(),\n",
        "t_u_train = train_t_un,\n",
        "t_u_val = val_t_un,\n",
        "t_c_train = train_t_c,\n",
        "t_c_val = val_t_c)\n",
        "seq_model.parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FP9sDjg49To3",
        "outputId": "71400838-82ff-4f44-e936-55fe146a8bcf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 0.1360, Validation loss 0.1196\n",
            "Epoch 10, Training loss 0.1353, Validation loss 0.1189\n",
            "Epoch 20, Training loss 0.1331, Validation loss 0.1168\n",
            "Epoch 30, Training loss 0.1293, Validation loss 0.1133\n",
            "Epoch 40, Training loss 0.1242, Validation loss 0.1085\n",
            "Epoch 50, Training loss 0.1179, Validation loss 0.1025\n",
            "Epoch 60, Training loss 0.1105, Validation loss 0.0954\n",
            "Epoch 70, Training loss 0.1021, Validation loss 0.0876\n",
            "Epoch 80, Training loss 0.0931, Validation loss 0.0792\n",
            "Epoch 90, Training loss 0.0837, Validation loss 0.0704\n",
            "Epoch 100, Training loss 0.0741, Validation loss 0.0615\n",
            "Epoch 110, Training loss 0.0646, Validation loss 0.0527\n",
            "Epoch 120, Training loss 0.0554, Validation loss 0.0444\n",
            "Epoch 130, Training loss 0.0469, Validation loss 0.0367\n",
            "Epoch 140, Training loss 0.0392, Validation loss 0.0299\n",
            "Epoch 150, Training loss 0.0326, Validation loss 0.0243\n",
            "Epoch 160, Training loss 0.0273, Validation loss 0.0200\n",
            "Epoch 170, Training loss 0.0235, Validation loss 0.0171\n",
            "Epoch 180, Training loss 0.0212, Validation loss 0.0159\n",
            "Epoch 190, Training loss 0.0206, Validation loss 0.0163\n",
            "Epoch 200, Training loss 0.0218, Validation loss 0.0185\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of Sequential(\n",
              "  (hidden_linear): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (hidden_activation): Tanh()\n",
              "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_model4(val_t_un)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvozswSGrDDp",
        "outputId": "a92cf854-21c2-4636-f361-47b4ba2f08de"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3978],\n",
              "        [0.3962],\n",
              "        [0.3968],\n",
              "        [0.3982],\n",
              "        [0.3988],\n",
              "        [0.3994],\n",
              "        [0.4009],\n",
              "        [0.3988],\n",
              "        [0.3989],\n",
              "        [0.3978],\n",
              "        [0.4006],\n",
              "        [0.3969],\n",
              "        [0.4023],\n",
              "        [0.3968],\n",
              "        [0.3996],\n",
              "        [0.3955],\n",
              "        [0.3987],\n",
              "        [0.3982],\n",
              "        [0.3995],\n",
              "        [0.3985],\n",
              "        [0.3964],\n",
              "        [0.3976],\n",
              "        [0.3981],\n",
              "        [0.3998],\n",
              "        [0.3974],\n",
              "        [0.3985],\n",
              "        [0.3988],\n",
              "        [0.4004],\n",
              "        [0.3988],\n",
              "        [0.4009],\n",
              "        [0.3988],\n",
              "        [0.3969],\n",
              "        [0.3965],\n",
              "        [0.3995],\n",
              "        [0.4001],\n",
              "        [0.3975],\n",
              "        [0.3975],\n",
              "        [0.3988],\n",
              "        [0.3995],\n",
              "        [0.4006],\n",
              "        [0.3987],\n",
              "        [0.3995],\n",
              "        [0.4033],\n",
              "        [0.3994],\n",
              "        [0.3975],\n",
              "        [0.3985],\n",
              "        [0.4006],\n",
              "        [0.4008],\n",
              "        [0.4007],\n",
              "        [0.3975],\n",
              "        [0.3994],\n",
              "        [0.3975],\n",
              "        [0.4010],\n",
              "        [0.4002],\n",
              "        [0.3986],\n",
              "        [0.4020],\n",
              "        [0.3956],\n",
              "        [0.3999],\n",
              "        [0.3996],\n",
              "        [0.3988],\n",
              "        [0.3994],\n",
              "        [0.3975],\n",
              "        [0.3995],\n",
              "        [0.3994],\n",
              "        [0.3975],\n",
              "        [0.3949],\n",
              "        [0.4004],\n",
              "        [0.3970],\n",
              "        [0.4005],\n",
              "        [0.3988],\n",
              "        [0.4010],\n",
              "        [0.4029],\n",
              "        [0.3975],\n",
              "        [0.3987],\n",
              "        [0.4016],\n",
              "        [0.3975],\n",
              "        [0.3962],\n",
              "        [0.3975],\n",
              "        [0.3981],\n",
              "        [0.3975],\n",
              "        [0.3975],\n",
              "        [0.3976],\n",
              "        [0.3988],\n",
              "        [0.3962],\n",
              "        [0.4016],\n",
              "        [0.3969],\n",
              "        [0.3974],\n",
              "        [0.3988],\n",
              "        [0.3962],\n",
              "        [0.3994],\n",
              "        [0.4004],\n",
              "        [0.3985],\n",
              "        [0.4012],\n",
              "        [0.3995],\n",
              "        [0.3990],\n",
              "        [0.4014],\n",
              "        [0.4006],\n",
              "        [0.3995],\n",
              "        [0.3968],\n",
              "        [0.3992],\n",
              "        [0.3994],\n",
              "        [0.3994],\n",
              "        [0.3997],\n",
              "        [0.3969],\n",
              "        [0.3984],\n",
              "        [0.3965],\n",
              "        [0.3986],\n",
              "        [0.3998],\n",
              "        [0.3981]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tun = 0.1 * tu\n",
        "\n",
        "torch.Tensor.ndim = property(lambda self: len(self.shape)) \n",
        "t_range = torch.arange(0., val_t_un.size(0)).unsqueeze(1)\n",
        "\n",
        "\n",
        "plt.xlabel(\"House Numbers\",color=\"black\")\n",
        "plt.ylabel(\"Actual House Prices\",color=\"black\")\n",
        "plt.plot(t_range.numpy(), seq_model(val_t_un).detach().numpy(),'r-')\n",
        "plt.plot(t_range.numpy(),val_t_c,'b-')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "5qKbFgiumzln",
        "outputId": "ad089f40-787e-4a0e-aa4f-30e190ece2de"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "torch.set_printoptions(edgeitems=2)\n",
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aWJIx1OKi9b",
        "outputId": "872eed22-b265-4f6c-8ac3-84e525d56bef"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9ca625c290>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']\n",
        "from torchvision import datasets, transforms\n",
        "data_path = '../data-unversioned/p1ch7/'\n",
        "cifar10 = datasets.CIFAR10(\n",
        "    data_path, train=True, download=False,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ],
      "metadata": {
        "id": "YOPRmJfrKol6"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10_val = datasets.CIFAR10(\n",
        "    data_path, train=False, download=False,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))\n",
        "label_map = {0: 0, 2: 1}\n",
        "class_names = ['airplane', 'bird']\n",
        "cifar2 = [(img, label_map[label])\n",
        "          for img, label in cifar10 \n",
        "          if label in [0, 2]]\n",
        "cifar2_val = [(img, label_map[label])\n",
        "              for img, label in cifar10_val\n",
        "              if label in [0, 2]]"
      ],
      "metadata": {
        "id": "noIbAjj6KsC6"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "n_out = 2\n",
        "\n",
        "model = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                3072,  # <1>\n",
        "                512,   # <2>\n",
        "            ),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(\n",
        "                512,   # <2>\n",
        "                n_out, # <3>\n",
        "            )\n",
        "        )\n",
        "def softmax(x):\n",
        "    return torch.exp(x) / torch.exp(x).sum()\n",
        "x = torch.tensor([1.0, 2.0, 3.0])\n",
        "\n",
        "softmax(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWJV39CgKu6D",
        "outputId": "5a7844bc-752c-4cff-e2fc-1415aa547cf9"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0900, 0.2447, 0.6652])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "            nn.Linear(3072, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 2),\n",
        "            nn.Softmax(dim=1))\n",
        "img, _ = cifar2[0]\n",
        "\n",
        "plt.imshow(img.permute(1, 2, 0))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "j414Y2v0K0Yo",
        "outputId": "85967e91-e8a5-4e26-ccc7-3ca7b5007922"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_batch = img.view(-1).unsqueeze(0)\n",
        "out = model(img_batch)\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jp14Qd1K5ye",
        "outputId": "b605f6ed-9709-4d5a-b187-a7c5b97c43e9"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4784, 0.5216]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, index = torch.max(out, dim=1)\n",
        "\n",
        "index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEUR0C20K8dD",
        "outputId": "258a6b1a-9e21-4c8b-a1b4-aee8ed4d1609"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = torch.tensor([\n",
        "    [0.6, 0.4],\n",
        "    [0.9, 0.1],\n",
        "    [0.3, 0.7],\n",
        "    [0.2, 0.8],\n",
        "])\n",
        "class_index = torch.tensor([0, 0, 1, 1]).unsqueeze(1)\n",
        "\n",
        "truth = torch.zeros((4,2))\n",
        "truth.scatter_(dim=1, index=class_index, value=1.0)\n",
        "truth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTS2Dq9GK-_e",
        "outputId": "9961bac2-cd60-4ef2-9d02-ba270f17f04e"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(out):\n",
        "    return ((out - truth) ** 2).sum(dim=1).mean()\n",
        "mse(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hSAncqqLDUx",
        "outputId": "b21c5f0d-d2d2-4e64-a522-16da0f0fb776"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1500)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out.gather(dim=1, index=class_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSbOqCedLEtS",
        "outputId": "58b9c135-8c60-4353-9b60-8ac4f89631d1"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6000],\n",
              "        [0.9000],\n",
              "        [0.7000],\n",
              "        [0.8000]])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def likelihood(out):\n",
        "    prod = 1.0\n",
        "    for x in out.gather(dim=1, index=class_index):\n",
        "        prod *= x\n",
        "    return prod\n",
        "\n",
        "likelihood(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILPf3OjfLG_8",
        "outputId": "31f7baeb-a479-4f72-bc60-544c9ee24082"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.3024])"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def neg_log_likelihood(out):\n",
        "    return -likelihood(out).log()\n",
        "\n",
        "neg_log_likelihood(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_O6wDWBLJG-",
        "outputId": "42e36af3-22da-4863-ca9e-8abcb6626931"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.1960])"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out0 = out.clone().detach()\n",
        "out0[0] = torch.tensor([0.9, 0.1]) # more right\n",
        "\n",
        "out2 = out.clone().detach()\n",
        "out2[0] = torch.tensor([0.4, 0.6]) # slightly wrong\n",
        "\n",
        "out3 = out.clone().detach()\n",
        "out3[0] = torch.tensor([0.1, 0.9]) # very wrong\n",
        "\n",
        "mse_comparison = torch.tensor([mse(o) for o in [out0, out, out2, out3]])\n",
        "mse_comparison"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzgnXG6sLLSP",
        "outputId": "3c45f12c-7d4f-4e7d-9b49-d99fd30580c5"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0750, 0.1500, 0.2500, 0.4750])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "((mse_comparison / mse_comparison[1]) - 1) * 100\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McwkiCg2LNuI",
        "outputId": "3d50ad0c-4866-49a9-8ddd-08616dc500a8"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-50.0000,   0.0000,  66.6667, 216.6667])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nll_comparison = torch.tensor([neg_log_likelihood(o) \n",
        "                               for o in [out0, out, out2, out3]])\n",
        "nll_comparison"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgY3JXNsLQBe",
        "outputId": "7586e673-823d-4aea-ef6e-126bd8196005"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7905, 1.1960, 1.6015, 2.9878])"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "((nll_comparison / nll_comparison[1]) - 1) * 100\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHHq0NMsLTMm",
        "outputId": "19064f34-995d-4c42-a58f-8ea249aaaa00"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-33.9016,   0.0000,  33.9016, 149.8121])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "\n",
        "log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "x = torch.tensor([[0.0, 104.0]])\n",
        "\n",
        "softmax(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPtbBGV9LWQc",
        "outputId": "686412da-aa3f-400d-e896-0bc3b86811c4"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.log(softmax(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HoT_Zu2LgHM",
        "outputId": "cc7d180f-461e-48ff-ccfa-5e603b939a3e"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-inf, 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "model = nn.Sequential(\n",
        "            nn.Linear(3072, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 2),\n",
        "            nn.LogSoftmax(dim=1))\n",
        "\n",
        "learning_rate = 1e-2\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "n_epochs = 200\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for img, label in cifar2:\n",
        "        out = model(img.view(-1).unsqueeze(0))\n",
        "        loss = loss_fn(out, torch.tensor([label]))\n",
        "                \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-jxJ-moLyQe",
        "outputId": "bd7fad0f-4609-4b26-f16b-48002b208189"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 8.307038\n",
            "Epoch: 1, Loss: 5.457431\n",
            "Epoch: 2, Loss: 3.882373\n",
            "Epoch: 3, Loss: 3.154704\n",
            "Epoch: 4, Loss: 11.443868\n",
            "Epoch: 5, Loss: 7.831904\n",
            "Epoch: 6, Loss: 0.055682\n",
            "Epoch: 7, Loss: 7.118860\n",
            "Epoch: 8, Loss: 6.412682\n",
            "Epoch: 9, Loss: 5.462630\n",
            "Epoch: 10, Loss: 6.324239\n",
            "Epoch: 11, Loss: 8.132204\n",
            "Epoch: 12, Loss: 4.478361\n",
            "Epoch: 13, Loss: 0.397974\n",
            "Epoch: 14, Loss: 3.924431\n",
            "Epoch: 15, Loss: 6.191980\n",
            "Epoch: 16, Loss: 4.834426\n",
            "Epoch: 17, Loss: 6.948432\n",
            "Epoch: 18, Loss: 0.884499\n",
            "Epoch: 19, Loss: 10.726386\n",
            "Epoch: 20, Loss: 8.787334\n",
            "Epoch: 21, Loss: 5.266410\n",
            "Epoch: 22, Loss: 7.096104\n",
            "Epoch: 23, Loss: 4.407344\n",
            "Epoch: 24, Loss: 6.470562\n",
            "Epoch: 25, Loss: 10.756291\n",
            "Epoch: 26, Loss: 16.045588\n",
            "Epoch: 27, Loss: 13.241747\n",
            "Epoch: 28, Loss: 8.135650\n",
            "Epoch: 29, Loss: 9.221145\n",
            "Epoch: 30, Loss: 11.256797\n",
            "Epoch: 31, Loss: 13.286485\n",
            "Epoch: 32, Loss: 14.590942\n",
            "Epoch: 33, Loss: 19.243477\n",
            "Epoch: 34, Loss: 7.613646\n",
            "Epoch: 35, Loss: 8.939101\n",
            "Epoch: 36, Loss: 5.921707\n",
            "Epoch: 37, Loss: 7.737715\n",
            "Epoch: 38, Loss: 10.683874\n",
            "Epoch: 39, Loss: 6.302479\n",
            "Epoch: 40, Loss: 15.454903\n",
            "Epoch: 41, Loss: 4.501896\n",
            "Epoch: 42, Loss: 0.014458\n",
            "Epoch: 43, Loss: 5.627636\n",
            "Epoch: 44, Loss: 13.283578\n",
            "Epoch: 45, Loss: 7.289899\n",
            "Epoch: 46, Loss: 17.841761\n",
            "Epoch: 47, Loss: 5.574378\n",
            "Epoch: 48, Loss: 16.024536\n",
            "Epoch: 49, Loss: 10.188716\n",
            "Epoch: 50, Loss: 8.278514\n",
            "Epoch: 51, Loss: 0.831824\n",
            "Epoch: 52, Loss: 0.090203\n",
            "Epoch: 53, Loss: 6.976837\n",
            "Epoch: 54, Loss: 5.525522\n",
            "Epoch: 55, Loss: 7.146000\n",
            "Epoch: 56, Loss: 10.842909\n",
            "Epoch: 57, Loss: 8.857367\n",
            "Epoch: 58, Loss: 11.958219\n",
            "Epoch: 59, Loss: 9.046844\n",
            "Epoch: 60, Loss: 13.173420\n",
            "Epoch: 61, Loss: 12.924408\n",
            "Epoch: 62, Loss: 7.175578\n",
            "Epoch: 63, Loss: 10.324291\n",
            "Epoch: 64, Loss: 18.187366\n",
            "Epoch: 65, Loss: 17.042473\n",
            "Epoch: 66, Loss: 15.801579\n",
            "Epoch: 67, Loss: 7.656318\n",
            "Epoch: 68, Loss: 6.750770\n",
            "Epoch: 69, Loss: 16.122250\n",
            "Epoch: 70, Loss: 15.059435\n",
            "Epoch: 71, Loss: 11.102892\n",
            "Epoch: 72, Loss: 16.944075\n",
            "Epoch: 73, Loss: 11.915767\n",
            "Epoch: 74, Loss: 7.378919\n",
            "Epoch: 75, Loss: 17.355034\n",
            "Epoch: 76, Loss: 13.972976\n",
            "Epoch: 77, Loss: 16.009201\n",
            "Epoch: 78, Loss: 15.350540\n",
            "Epoch: 79, Loss: 23.867161\n",
            "Epoch: 80, Loss: 27.666374\n",
            "Epoch: 81, Loss: 8.405586\n",
            "Epoch: 82, Loss: 4.449419\n",
            "Epoch: 83, Loss: 11.115731\n",
            "Epoch: 84, Loss: 3.102489\n",
            "Epoch: 85, Loss: 0.083001\n",
            "Epoch: 86, Loss: 5.213762\n",
            "Epoch: 87, Loss: 10.287248\n",
            "Epoch: 88, Loss: 3.234745\n",
            "Epoch: 89, Loss: 8.348825\n",
            "Epoch: 90, Loss: 0.771752\n",
            "Epoch: 91, Loss: 10.688971\n",
            "Epoch: 92, Loss: 6.329175\n",
            "Epoch: 93, Loss: 7.013631\n",
            "Epoch: 94, Loss: 0.126584\n",
            "Epoch: 95, Loss: 1.748496\n",
            "Epoch: 96, Loss: 0.000046\n",
            "Epoch: 97, Loss: 18.159552\n",
            "Epoch: 98, Loss: 16.878725\n",
            "Epoch: 99, Loss: 19.723660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "        \n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bYSbTPqbxSz",
        "outputId": "fc86c280-35f4-4fe9-a1f9-1fa9e93b12b4"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.999200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "\n",
        "model = nn.Sequential(\n",
        "            nn.Linear(3072, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 2),\n",
        "            nn.LogSoftmax(dim=1))\n",
        "\n",
        "learning_rate = 1e-2\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "n_epochs = 200\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJcRH7DcL-hJ",
        "outputId": "8d96b336-b74f-4af9-a19b-cee4a074dbf4"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 0.447263\n",
            "Epoch: 1, Loss: 0.417204\n",
            "Epoch: 2, Loss: 0.288800\n",
            "Epoch: 3, Loss: 0.329748\n",
            "Epoch: 4, Loss: 0.442465\n",
            "Epoch: 5, Loss: 0.406243\n",
            "Epoch: 6, Loss: 0.698463\n",
            "Epoch: 7, Loss: 0.429398\n",
            "Epoch: 8, Loss: 0.375769\n",
            "Epoch: 9, Loss: 0.517260\n",
            "Epoch: 10, Loss: 0.428689\n",
            "Epoch: 11, Loss: 0.416207\n",
            "Epoch: 12, Loss: 0.365555\n",
            "Epoch: 13, Loss: 0.329109\n",
            "Epoch: 14, Loss: 0.320614\n",
            "Epoch: 15, Loss: 0.416643\n",
            "Epoch: 16, Loss: 0.434464\n",
            "Epoch: 17, Loss: 0.323721\n",
            "Epoch: 18, Loss: 0.296406\n",
            "Epoch: 19, Loss: 0.525408\n",
            "Epoch: 20, Loss: 0.275151\n",
            "Epoch: 21, Loss: 0.296652\n",
            "Epoch: 22, Loss: 0.370118\n",
            "Epoch: 23, Loss: 0.189625\n",
            "Epoch: 24, Loss: 0.260521\n",
            "Epoch: 25, Loss: 0.231848\n",
            "Epoch: 26, Loss: 0.109705\n",
            "Epoch: 27, Loss: 0.537064\n",
            "Epoch: 28, Loss: 0.154275\n",
            "Epoch: 29, Loss: 0.526165\n",
            "Epoch: 30, Loss: 0.219033\n",
            "Epoch: 31, Loss: 0.316020\n",
            "Epoch: 32, Loss: 0.287425\n",
            "Epoch: 33, Loss: 0.222993\n",
            "Epoch: 34, Loss: 0.198543\n",
            "Epoch: 35, Loss: 0.110400\n",
            "Epoch: 36, Loss: 0.185364\n",
            "Epoch: 37, Loss: 0.136970\n",
            "Epoch: 38, Loss: 0.430483\n",
            "Epoch: 39, Loss: 0.090185\n",
            "Epoch: 40, Loss: 0.218461\n",
            "Epoch: 41, Loss: 0.054010\n",
            "Epoch: 42, Loss: 0.180352\n",
            "Epoch: 43, Loss: 0.063841\n",
            "Epoch: 44, Loss: 0.080691\n",
            "Epoch: 45, Loss: 0.160095\n",
            "Epoch: 46, Loss: 0.132637\n",
            "Epoch: 47, Loss: 0.096404\n",
            "Epoch: 48, Loss: 0.131084\n",
            "Epoch: 49, Loss: 0.056673\n",
            "Epoch: 50, Loss: 0.148022\n",
            "Epoch: 51, Loss: 0.100497\n",
            "Epoch: 52, Loss: 0.123579\n",
            "Epoch: 53, Loss: 0.118554\n",
            "Epoch: 54, Loss: 0.102245\n",
            "Epoch: 55, Loss: 0.065565\n",
            "Epoch: 56, Loss: 0.055163\n",
            "Epoch: 57, Loss: 0.106877\n",
            "Epoch: 58, Loss: 0.096717\n",
            "Epoch: 59, Loss: 0.070170\n",
            "Epoch: 60, Loss: 0.081440\n",
            "Epoch: 61, Loss: 0.058894\n",
            "Epoch: 62, Loss: 0.078915\n",
            "Epoch: 63, Loss: 0.240777\n",
            "Epoch: 64, Loss: 0.053418\n",
            "Epoch: 65, Loss: 0.032838\n",
            "Epoch: 66, Loss: 0.022308\n",
            "Epoch: 67, Loss: 0.072340\n",
            "Epoch: 68, Loss: 0.023792\n",
            "Epoch: 69, Loss: 0.021565\n",
            "Epoch: 70, Loss: 0.023792\n",
            "Epoch: 71, Loss: 0.035432\n",
            "Epoch: 72, Loss: 0.111931\n",
            "Epoch: 73, Loss: 0.055290\n",
            "Epoch: 74, Loss: 0.029029\n",
            "Epoch: 75, Loss: 0.030481\n",
            "Epoch: 76, Loss: 0.093939\n",
            "Epoch: 77, Loss: 0.024223\n",
            "Epoch: 78, Loss: 0.033667\n",
            "Epoch: 79, Loss: 0.028950\n",
            "Epoch: 80, Loss: 0.027209\n",
            "Epoch: 81, Loss: 0.031006\n",
            "Epoch: 82, Loss: 0.039555\n",
            "Epoch: 83, Loss: 0.024003\n",
            "Epoch: 84, Loss: 0.019893\n",
            "Epoch: 85, Loss: 0.011406\n",
            "Epoch: 86, Loss: 0.018686\n",
            "Epoch: 87, Loss: 0.021994\n",
            "Epoch: 88, Loss: 0.016676\n",
            "Epoch: 89, Loss: 0.023618\n",
            "Epoch: 90, Loss: 0.027187\n",
            "Epoch: 91, Loss: 0.047341\n",
            "Epoch: 92, Loss: 0.034266\n",
            "Epoch: 93, Loss: 0.020675\n",
            "Epoch: 94, Loss: 0.024404\n",
            "Epoch: 95, Loss: 0.019039\n",
            "Epoch: 96, Loss: 0.010981\n",
            "Epoch: 97, Loss: 0.006711\n",
            "Epoch: 98, Loss: 0.031388\n",
            "Epoch: 99, Loss: 0.013551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anQvjVwlZNA3",
        "outputId": "5b256f27-c0d6-4844-d020-9a5c59d61675"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.999200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "        \n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKHg4wPKZTa3",
        "outputId": "6a206872-3a02-44bf-c050-1582a0bd6367"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.814000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "model = nn.Sequential(\n",
        "            nn.Linear(3072, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 2),\n",
        "            nn.LogSoftmax(dim=1))\n",
        "\n",
        "learning_rate = 1e-2\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "n_epochs = 200\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for img, label in cifar2:\n",
        "        out = model(img.view(-1).unsqueeze(0))\n",
        "        loss = loss_fn(out, torch.tensor([label]))\n",
        "                \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as2SZRwiZWea",
        "outputId": "3363a859-7358-490d-bac0-c3b431d08adf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 1.682307\n",
            "Epoch: 1, Loss: 1.684133\n",
            "Epoch: 2, Loss: 2.350468\n",
            "Epoch: 3, Loss: 2.789493\n",
            "Epoch: 4, Loss: 2.271224\n",
            "Epoch: 5, Loss: 2.754690\n",
            "Epoch: 6, Loss: 3.296408\n",
            "Epoch: 7, Loss: 4.098645\n",
            "Epoch: 8, Loss: 4.402999\n",
            "Epoch: 9, Loss: 1.851165\n",
            "Epoch: 10, Loss: 2.079173\n",
            "Epoch: 11, Loss: 6.380314\n",
            "Epoch: 12, Loss: 3.186968\n",
            "Epoch: 13, Loss: 3.156041\n",
            "Epoch: 14, Loss: 3.502503\n",
            "Epoch: 15, Loss: 4.139131\n",
            "Epoch: 16, Loss: 3.172513\n",
            "Epoch: 17, Loss: 1.838601\n",
            "Epoch: 18, Loss: 2.050254\n",
            "Epoch: 19, Loss: 4.537967\n",
            "Epoch: 20, Loss: 6.992655\n",
            "Epoch: 21, Loss: 5.817654\n",
            "Epoch: 22, Loss: 5.038824\n",
            "Epoch: 23, Loss: 8.134530\n",
            "Epoch: 24, Loss: 2.972916\n",
            "Epoch: 25, Loss: 3.656368\n",
            "Epoch: 26, Loss: 1.646843\n",
            "Epoch: 27, Loss: 6.410490\n",
            "Epoch: 28, Loss: 6.104948\n",
            "Epoch: 29, Loss: 0.318380\n",
            "Epoch: 30, Loss: 1.227196\n",
            "Epoch: 31, Loss: 1.113218\n",
            "Epoch: 32, Loss: 0.820174\n",
            "Epoch: 33, Loss: 1.900748\n",
            "Epoch: 34, Loss: 4.016766\n",
            "Epoch: 35, Loss: 2.837506\n",
            "Epoch: 36, Loss: 0.353581\n",
            "Epoch: 37, Loss: 1.879092\n",
            "Epoch: 38, Loss: 5.882475\n",
            "Epoch: 39, Loss: 3.236743\n",
            "Epoch: 40, Loss: 1.477631\n",
            "Epoch: 41, Loss: 1.968538\n",
            "Epoch: 42, Loss: 4.179394\n",
            "Epoch: 43, Loss: 2.858269\n",
            "Epoch: 44, Loss: 8.844487\n",
            "Epoch: 45, Loss: 3.249692\n",
            "Epoch: 46, Loss: 6.797155\n",
            "Epoch: 47, Loss: 1.072914\n",
            "Epoch: 48, Loss: 0.418369\n",
            "Epoch: 49, Loss: 7.962488\n",
            "Epoch: 50, Loss: 1.839746\n",
            "Epoch: 51, Loss: 5.219704\n",
            "Epoch: 52, Loss: 4.283232\n",
            "Epoch: 53, Loss: 0.038385\n",
            "Epoch: 54, Loss: 0.384137\n",
            "Epoch: 55, Loss: 0.160668\n",
            "Epoch: 56, Loss: 0.543250\n",
            "Epoch: 57, Loss: 3.046757\n",
            "Epoch: 58, Loss: 0.706484\n",
            "Epoch: 59, Loss: 0.034583\n",
            "Epoch: 60, Loss: 3.894212\n",
            "Epoch: 61, Loss: 2.904253\n",
            "Epoch: 62, Loss: 3.955839\n",
            "Epoch: 63, Loss: 5.872950\n",
            "Epoch: 64, Loss: 6.928630\n",
            "Epoch: 65, Loss: 4.600848\n",
            "Epoch: 66, Loss: 2.775519\n",
            "Epoch: 67, Loss: 4.137508\n",
            "Epoch: 68, Loss: 0.861638\n",
            "Epoch: 69, Loss: 3.666455\n",
            "Epoch: 70, Loss: 0.673347\n",
            "Epoch: 71, Loss: 8.787339\n",
            "Epoch: 72, Loss: 3.519139\n",
            "Epoch: 73, Loss: 0.044193\n",
            "Epoch: 74, Loss: 1.846625\n",
            "Epoch: 75, Loss: 3.994903\n",
            "Epoch: 76, Loss: 7.221816\n",
            "Epoch: 77, Loss: 3.813334\n",
            "Epoch: 78, Loss: 1.408703\n",
            "Epoch: 79, Loss: 2.691728\n",
            "Epoch: 80, Loss: 6.171129\n",
            "Epoch: 81, Loss: 7.536922\n",
            "Epoch: 82, Loss: 2.135471\n",
            "Epoch: 83, Loss: 9.277877\n",
            "Epoch: 84, Loss: 3.783490\n",
            "Epoch: 85, Loss: 0.419074\n",
            "Epoch: 86, Loss: 10.685421\n",
            "Epoch: 87, Loss: 13.745734\n",
            "Epoch: 88, Loss: 4.055562\n",
            "Epoch: 89, Loss: 2.959330\n",
            "Epoch: 90, Loss: 4.762715\n",
            "Epoch: 91, Loss: 0.878582\n",
            "Epoch: 92, Loss: 4.177922\n",
            "Epoch: 93, Loss: 10.901397\n",
            "Epoch: 94, Loss: 13.368613\n",
            "Epoch: 95, Loss: 4.562441\n",
            "Epoch: 96, Loss: 5.183382\n",
            "Epoch: 97, Loss: 6.717193\n",
            "Epoch: 98, Loss: 10.970145\n",
            "Epoch: 99, Loss: 0.077002\n",
            "Epoch: 100, Loss: 3.709467\n",
            "Epoch: 101, Loss: 0.711914\n",
            "Epoch: 102, Loss: 2.299769\n",
            "Epoch: 103, Loss: 1.024065\n",
            "Epoch: 104, Loss: 8.570540\n",
            "Epoch: 105, Loss: 4.712903\n",
            "Epoch: 106, Loss: 4.328175\n",
            "Epoch: 107, Loss: 5.543203\n",
            "Epoch: 108, Loss: 9.420440\n",
            "Epoch: 109, Loss: 0.061269\n",
            "Epoch: 110, Loss: 7.002592\n",
            "Epoch: 111, Loss: 12.711350\n",
            "Epoch: 112, Loss: 1.733906\n",
            "Epoch: 113, Loss: 15.373075\n",
            "Epoch: 114, Loss: 0.314139\n",
            "Epoch: 115, Loss: 0.735043\n",
            "Epoch: 116, Loss: 2.388031\n",
            "Epoch: 117, Loss: 9.940347\n",
            "Epoch: 118, Loss: 2.786835\n",
            "Epoch: 119, Loss: 0.001461\n",
            "Epoch: 120, Loss: 6.814554\n",
            "Epoch: 121, Loss: 5.647228\n",
            "Epoch: 122, Loss: 0.003502\n",
            "Epoch: 123, Loss: 7.343244\n",
            "Epoch: 124, Loss: 0.923094\n",
            "Epoch: 125, Loss: 0.000013\n",
            "Epoch: 126, Loss: 0.563968\n",
            "Epoch: 127, Loss: 0.000080\n",
            "Epoch: 128, Loss: 0.042697\n",
            "Epoch: 129, Loss: 13.368402\n",
            "Epoch: 130, Loss: 4.940992\n",
            "Epoch: 131, Loss: 0.108958\n",
            "Epoch: 132, Loss: 1.729630\n",
            "Epoch: 133, Loss: 3.478849\n",
            "Epoch: 134, Loss: 8.510243\n",
            "Epoch: 135, Loss: 16.514650\n",
            "Epoch: 136, Loss: 15.007195\n",
            "Epoch: 137, Loss: 8.007205\n",
            "Epoch: 138, Loss: 4.751171\n",
            "Epoch: 139, Loss: 0.094530\n",
            "Epoch: 140, Loss: 14.170504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "        \n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ],
      "metadata": {
        "id": "R-4Atehyb1Bt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}